\section{Introduction}
\emph{XIST} and \emph{RSX} provide compelling examples of how the sequence-to-function relationship in lncRNAs may be modular in nature. These two transcripts demonstrate clear sequence regions, defined by the boundaries of their tandem repeat domains, that are conserved and are clearly functional as demonstrated by the binding of RNA binding proteins to their transcripts. Our model of lncRNA function, specifically the bag-of-words model that the order of short motifs in a sequence is less important than their overall density, implicitly means that a tandem repeated sequence may not be essential for recruitment of these RNA binding proteins.

It is possible to show that the $k$-mer content of a sequence can be perfectly preserved while destroying the tandomly repeated nature of a sequence (Fig 3.1 A-C). This can be done by building a graph whose nodes are $(k-1)$-mers and whose edges are connected such that a sequence is reconstructed with the exact same $k$-mer frequencies as the original sequence ($\mu$Shuffle). Furthermore, if we examine lncRNA sequences in mouse with very high SEEKR correlation to HXD it is clear that not all these sequences contain substantial tandemly repeated sequence as in \emph{XIST} (Fig 3.1 D-H). 
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{shufD.png}
\caption{Dot plot alignments and SEEKR-defined similarity between human Xist Repeat D, shuffled Repeat D, and the top 5 mouse lncRNAs that are most similar to Repeat D. Repeat D was shuffled using $\mu$Shuffle and preserving $k$-mer content at $k$ = 4 (Jiang et al., 2008). Dotplots were generated using a window size 20 nucleotides and a threshold of 50\% identity.}
\label{fig:shufD}
\end{figure}

Therefore, a challenge in understanding lncRNA function is identifying where functional modules may be located within a sequence. Clearly, the answer is not solely exonic sequence as in a protein coding reading frame. Within \emph{XIST} these sequence regions are sub-sequences within the spliced transcript. Compounding the issue is that as far as is known, no specific boundary motifs are required or even found. Potential splice junctions in genes are relatively easy to identify based off a combination of sequence content, and the presence of conserved 5' and 3' splice site motifs that demarcate the beginning and end of an intron, respectively. 

To model this hypothesized sequence structure, we have developed an HMM that is designed to identify sub-sequences within a larger transcript that contain regions of elevated $k$-mer content to a known functional domain. Given how little is known in the field about the sequence-to-function relationship in lncRNAs, we use the archetypal lncRNA \emph{XIST} and its known functional sub-sequences as our model. 
\section{Results}
\subsection{Model Structure}
Given a transcript sequence, $X$, of length $L$, and a set of predefined functional features $Y$, the objective is to map each nucleotide $x \in X$ to a functional state, $y \in Y$. Our underlying biological hypothesis is that non-coding RNAs contain sub-sequences of specific $k$-mer content that is enriched for motifs that bind a specific RBP or subset of RBPs and that these domains are free to start and stop within a larger sequence. We therefore defined the first functional feature to be the \emph{query}, which is a categorical distribution of $k$-mer frequencies from training sequences known to have some specific functional role (e.g. tandem repeats of \emph{XIST}). We defined one additional functional feature which comprises a \emph{null} feature. The null state is a categorical distribution of $k$-mer frequencies that represent the average of the transcriptome, or background sequence. 

From a probabilistic point of view, we assume there is an underlying stochastic process generating these functional domains and placing them in a sequence, and that these functional domains are then generating the sequence that we observe in $X$. Prior HMM based models have primarily modeled the emission of a sequence of individual nucleotides and incorporated high-order interactions through conditional probabilities. 

As an example, if we observed the following sequence, we could calculate the probability of that sequence using varying amounts of prior information:

\begin{table}[h!]
\centering
 \begin{tabular}{|c | c| c |}
 \hline
 \multicolumn{3}{|c|}{Observed Sequence $X = ATCGA$}\\
 \hline
 Markov Order & P(X) & Parameters\\
 \hline\hline
 0 & $P(A)P(T)P(C)P(G)P(A)$ & 4 \\ 
 \hline
 1 & $P(A)P(T|A)P(C|T)P(G|C)P(A|G)$ & 16\\
 \hline
 2 & $P(A)P(T|A)P(C|AT)P(G|TC)P(A|CG)$ & 64 \\
 \hline
 3 & $P(A)P(T|A)P(C|AT)P(G|ATC)P(A|TCG)$ & 256 \\
 \hline
 
\end{tabular}
\caption{Progressively more prior history can be incorporated to model the probability distribution of a sequence, however doing so exponentially increases the number of parameters to be estimated.}
\label{table:1}
\end{table}


Given the underlying assumptions of SEEKR we chose to model the emissions of $k$-mers directly rather than individual nucleotides conditioned on $k$ previous nucleotides. When constructing a parse of a sequence, this choice reconstructs the higher-order motif context while allowing each emission to be conditionally dependent only on the functional feature emitting it (Figure 3.2).

For example, if $k=5$, then the probability of observing the $5$-mer $ATCGA$ is equivalent to the probability of observing $A$ given the probability of the preceeding nucleotides $ATCG$, multiplied by the probability of observing $ATCG$. 

$$P(ATCGA) = P(A|ATCG)P(ATCG)$$

The last modeling choice we make is to construct the HMM such that each hidden state $y$ has a non-zero probability of transitioning to any other state, including itself. This is formally known as an ergodic HMM. Prior HMM models of DNA sequences are primarily left-right structured, due to prior knowledge of the structure of a gene, or motif. If a model for a gene were constructed, the transition Intron $\rightarrow$ 5' UTR can be assigned a probability of zero given prior knowledge. Within lncRNAs, we and others hypothesized and shown that protein binding functional domains are modular in nature and therefore the hidden states within our HMM are free to transition.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{model.png}
\caption{}
%\label{fig:x cubed graph}
\end{figure}

\subsection{Parameter Estimation}
A hidden markov model is comprised of a stochastic transition matrix $A$, where each row vector $\phi_i$ corresponds to a categorical distribution representing hidden state $i$ probability of transitioning to state $j$. The emission matrix $E$, where each row corresponds to a categorical distribution of $k$-mer frequencies. Finally, there is the initialization matrix $\pi$ that describes the probability of starting at each hidden state at $t=1$.
\subsubsection{Emission Parameters}
We define the emission distribution for the query and null hidden states to be $k$-mer frequencies taken from a set of training sequences. For the query hidden state, we defined our training set to be \emph{XIST} repeats A,B,D, and E. For the null hidden state, we defined the set of training sequences to be the set of unspliced lncRNAs in the human transcriptome. 

\begin{table}[h]
\centering
\begin{center}
 \begin{tabular}{|c | c| c | c |} 
 \hline
 Query & k & $\alpha$ & $\beta$ \\
 \hline\hline
 Repeat A & 4 & .9999 & .9999 \\ 
 \hline
 Repeat B & 4 & .9999 & .9999\\
 \hline
 Repeat D & 2 & .75 & .9999\\
 \hline
 Repeat E & 4 & .5 & .9999\\
 \hline
\end{tabular}
\end{center}
\caption{Best values of k, the $Q\rightarrow Q$ parameter $\alpha$, and $N\rightarrow N$ parameter $\beta$ as determined through a grid search based approach to identify sequences with the highest SEEKR correlation to the query.}
\label{table:1}
\end{table}

The value of $k$ for each query was determined using a grid search based approach outlined in the Methods section below. Some of the \emph{XIST} repeats are quite short, for example \emph{XIST} repeat B is only $\approx 200$bp in length. Therefore, it is often the case that there are fewer nucleotides than there are parameters to estimate leading to zeros in the probability distribution. This is both unlikely, as many of the motifs for RBPs that bind \emph{XIST} have no non-zero probabilities, and also mathematically untractable, as we will need to work in log-space for probability calculations later.

To correct for this we employed pseudo-counts, specifically we use a $+1$ pseudo-count for all $k$-mers in the training set (\emph{i.e.} initialize the counts array to 1). The justification for this is relatively straightforward and rigorous. Given the transition matrix $A$, we know that each row vector within it is a categorical distribution $\phi$ comprised of all $k$-mer frequencies. Our hypothesis is that in spite of our limited training data, we believe that there is an underlying randomness, or probability distribution, to these $k$-mer frequencies, and we have not been able to sample enough sequence to account for that stochasticity. 

It can be shown that the \emph{maximum a posteriori}, or MAP, estimate for our $k$-mer frequency distribution is achieved by adding a pseudo-count of $1$ to each $k$-mer. This is achieved by placing a Dirichlet prior on our $k$-mer frequency distribution:

\begin{center}
    $\phi_i \sim Dir(\alpha_{AAA},\alpha_{AAT},\dots,\alpha_{GGG})$
    
    $x_i \sim Cat(\phi_{AAA},\phi_{AAT},\dots,\phi_{GGG})$
\end{center}

That is to say, the best estimate of the $k$-mer frequencies, given the data we have available and the uncertainty we have in it, is achieved by adding a count of $1$ to all $k$-mers. Intuitively this makes sense, as this addition has the largest impact when we have very little data to work with, and very little impact when we have substantial data as is the case for the null hidden state.

\subsubsection{Transition Parameters}

We are using the SEEKR algorithm to calculate which set of parameters for our HMM maximize the ‘distance’, or Kullback-Leibler diveregence, between a distribution of parsed and unparsed sequences.
The KL-Divergence is defined as:
\begin{equation}
    D_{KL}(P||Q) = \sum_{x\in X}P(x)\log{\frac{P(x)}{Q(x}}
\end{equation}
Where $D_{KL}$ is the KL-Divergence between two probability distribtions P and Q. Here we will let P represent the SEEKR score distribution of the HMM parse hits to the query sequence, and Q represent the SEEKR score distribution of unparsed sequences to the query sequence, representing a ‘reference’. If P = Q then DKL = 0. D increases as the probability distributions become more dissimilar to each other. Important to note that $D_{KL}(P||Q) \neq D_{KL}(Q||P)$, so for consistenty $D_{KL}(P||Q)$ is always calculated.
Our goal is to find a set of parameters, $\alpha$ ($Q\rightarrow Q$), $\beta$ ($N\rightarrow N$), and k, that maximize the above expression
\begin{equation}
    \argmax_{\alpha,\beta,k}\sum_{x\in X}P(x|\alpha,\beta,k)\log{\frac{P(x|\alpha,\beta,k)}{Q(x|k)}}
\end{equation}
Being comprised of unparsed sequences, $Q(x)$ just depends on k.
In the real dataset, $P$ and $Q$ are histograms calculated from the data, and so the calculation of $D_{KL}$ compares the frequencies of SEEKR correlations in each bin for the two distributions. 

We found that $\beta$ must always be close to 1, whereas significant variance was found for the value of $\alpha$ amongst the different \emph{XIST} queries. We also found that there was often significant overlap of hits between the varying sets of parameters (Fig 3.3)

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/intersection.pdf}
\caption{Intersecting bed files of different parameters... fill out later.}
%\label{fig:x cubed graph}
\end{figure}

\subsection{Viterbi Parsing and Scoring}
The primary use for the HMM in \emph{hmmSEEKR} is identifying the most likely parse, $\phi_{i}$ for a given sequence or set of sequences, $X_i \in \{X_1,X_2,\dots,X_n\}$. The regions that we define as ``hits" are all sub-sequences of contiguously query-labeled sequence, \emph{e.g.} for a sequence/parse pair such as:

$$(X_i=ATCGCCCG,\phi_i=-,-,+,+,+,+,+,+)$$

The ``hit" in this example is defined as the sequence CGCCCG. There may be any number of hits within a sequence, but a hit is always composed of contiguous ``$+$" (query) labeled nucleotides.  The Viterbi path through each sequence was calculated as in algorithm \ref{alg:viterbi}, \ref{alg:backtrack}, and was implemented in corefunctions.py within the \emph{hmmSEEKR} package. This code can be called as in section 3.4.3 within the mSEEKR.py program.

Table \ref{tbl:hmmresults} illustrates the output of mSEEKR.py when scanning mouse \emph{Xist} using a human \emph{XIST} query. Here, an HMM was trained on human \emph{XIST} repeat A as the query hidden state, and the null hidden state was trained on the set of all unspliced lncRNAs in  mice (Methods 3.4.3), and mouse \emph{Xist} was scanned for matches to the repeat A query. In the literature, repeat A is defined in mouse as spanning basepairs 292-713 (brockdorff), and \emph{hmmSEEKR} called basepairs 201-748 as a hit to the repeat A query within mouse Xist (Table \ref{tbl:hmmresults}), yielding 100\% recovery of the original sequence. The score, kmerLLR as in table \ref{tbl:hmmresults}, is calculated as in algorithm \ref{alg:LLR}. As the original definition for repeat A was formally defined by the presence of the tandem repeat, and not based on functional motif enrichment, it is harder to accurately assess the false positive rate of the HMM.


\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline 
Rank&Start & End   & Length & kmerLLR & seqName                               \\
\hline 
0     & 201   & 748    & 547     & 285.225  & \textgreater{}xist \\
1     & 10295 & 11052  & 757     & 42.734   & \textgreater{}xist \\
2     & 1532  & 1576   & 44      & 30.685  & \textgreater{}xist \\
3     & 11326 & 11450  & 124     & 29.306   & \textgreater{}xist \\
4     & 17941 & 17946  & 5       & -0.968 & \textgreater{}xist\\
\hline 
\end{tabular}
\caption{Test}
\label{tbl:hmmresults}
\end{table}

As there are no pre-existing annotations for these $k$-mer based sequence features within lncRNAs, it is difficult to define what constitutes a true positive and a true negative. To help elucidate what sequence features within \emph{XIST} and other lncRNAs are functional, we turned to protein binding data for proteins that are known to bind \emph{XIST} and have been shown to be crucial for its function.

\subsection{Xist Associated RBPs}
A crucial feature of the \emph{XIST} transcript are the 4 core repeats that bind unique subsets of RNA binding proteins. Several RBPs have been shown to be crucial for the function of \emph{XIST}, including HNRNPK, RBM15, and several others. The tandem repeats have been defined based off their repetitiveness, but we have shown in Figure \ref{fig:shufD} that a non-repetitive sequence can have the same motif content as a repetitive one. Therefore, we hypothesized that the functional elements of lncRNA such as \emph{XIST} are not defined by repetitiveness but rather by enrichment of protein binding motifs. To further test \emph{hmmSEEKR}, we sought to identify the proteins most associated with the A,B,D,E-repeats of \emph{XIST} so that we could validate the predictive power of our model against existing protein binding data.
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{images/sigproteins.pdf}
\caption{Proteins selectively enriched within tandem repeats of \emph{XIST}. A) $\log_2$ ratio of read density for Repeat A compared to the remainder of \emph{XIST} is shown. Individual points represent biological replicates. B) $\log_2$ ratio of read density for Repeat B compared to the remainder of \emph{XIST} is shown. Individual points represent biological replicates. C) $\log_2$ ratio of read density for Repeat D compared to the remainder of \emph{XIST} is shown. Individual points represent biological replicates. D) $\log_2$ ratio of read density for Repeat E compared to the remainder of \emph{XIST} is shown. Individual points represent biological replicates.}
\label{fig:xistproteins}
\end{figure}

We found that each repeat within \emph{XIST} contained at least one protein that was at least 2x more enriched in within the repeat than the remainder of the \emph{XIST} transcript (Figure \ref{fig:xistproteins}A). RBM15 and SRSF1 were found to be the most specific proteins to repeat A. Prior work has shown that RBM15 is required for full \emph{XIST} silencing functionality, and that RBM15 recruits the m6A complex to the 5' region of \emph{XIST}. SRSF1 has previously been shown to bind the A-repeat of \emph{XIST} but its function within the sequence is currently unknown.

Both the B-repeat and D-repeat were found to be signficantly enriched for HNRNPK over the rest of the transcript (Figure \ref{fig:xistproteins}B-C). HNRNPK is required for recruitment of PRC1 to the \emph{XIST} transcripts. Deletion of the B-repeat region has previously been shown to be sufficient to abrogate \emph{XIST} dependent PRC recruitment, as well as \emph{XIST} mediated silencing. Our analysis showed that DROSHA is also enriched within the B-repeat of \emph{XIST}, however no previous studies have identified DROSHA as a direct interactor with \emph{XIST} and so the function of this interaction is unknown.

The E-repeat of \emph{XIST} was highly enriched for several proteins. The E-repeat itself is composed primarily of T-rich sequence, and therefore the proteins we observe often contain similar motifs (cisBP). PTBP1, MATR3, TARDBP, and TIA1 were all found to be the most enriched within the E-repeat. PTBP1 has previously been shown to be required for proper \emph{XIST} expression and splicing during development. TARDBP depletion is also associated with increased expression of mis-spliced \emph{XIST} transcripts. A recent study found that MATR3 and PTBP1 form a condensate mediated by the \emph{XIST} E-repeat and this condensate is required for \emph{XIST} localization to the Xi. 

\subsection{Detection of Rsx Domains}
Our \emph{Rsx} study in Chapter 2 relied on the existence of tandem repeats within the \emph{Rsx} transcript to perform the domain based SEEKR analysis. On a whole transcript level, \emph{XIST} and \emph{Rsx} were slightly anti-correlated despite their shared function. The relationship between the two sequences didn't become apparent until we parsed out the tandem repeats within \emph{Rsx} and performed pairwise comparisons between each \emph{XIST} tandem repeat. 

We hypothesized that we could use \emph{hmmSEEKR} to identify regions of non-linear sequence similarity between \emph{XIST} and \emph{Rsx} without \emph{a priori} identification of tandem repeats. To do this, we trained 4 separate HMMs on the A,B,D, and E-repeats of \emph{XIST} at $k=2,3,4,5,6$ and all pairwise combinations of $\alpha,\beta = .5,.75,.9,.99,.999,.9999$. The results outlined in Figure \ref{fig:koalarsxhmm} and Table \ref{tbl:rsxresults} are using the set of parameters that yielded the best F1 score (Methods 3.4.X). 


\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{images/koalarsx.pdf}
\caption{Proteins selectively enriched within tandem repeats of \emph{XIST}. A) $\log_2$ ratio of read density for Re}
\label{fig:koalarsxhmm}
\end{figure}

We found that the HMM trained on the \emph{XIST} D-repeat captured the entire tandem repeat from transcript coordinates 1,000-14,000 (Figure \ref{fig:koalarsxhmm}, Table \ref{tbl:rsxresults}). Likewise, the HMM trained on the \emph{XIST} B-repeat captured a significant portion of \emph{Rsx} repeat 1, and these regions were significantly more C-rich than the remainder of \emph{Rsx} repeat 1 (show data). Additional C-rich sequence were also identified at the 5' and 3' regions of \emph{Rsx} (Figure \ref{fig:koalarsxhmm}). The HMMs trained on the A-repeat and E-repeat both significantly aligned with \emph{Rsx} repeat 4, with 100\% precision and 25.45\% recall. In our original analysis, we mapped A,E-repeats to repeats 2,3, and 4 in \emph{Rsx}, however repeats 2 and 3 had relatively marginal correlation to the A,E-repeats, whereas the A,E$\rightarrow$4 relationship was the strongest of all comparisons in the original \emph{Rsx} analysis (Sprague et al.).

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline 
\emph{Rsx} Repeat Name& Start & End   & Xist Association   & Precision& Recall                      \\
\hline 
    1&  1000   & 14000    & B,D     & 98.42\% & 100\% \\
      2&17500 & 21000  & A,E     & 100\%   & 1.32\% \\
      3&21500  & 22500   & A,E      & 0\% & 0\%  \\
      4&23000 & 27500  & A,E     & 100\%   & 25.45\% \\
\hline 
\end{tabular}
\caption{Test}
\label{tbl:rsxresults}
\end{table}
\subsection{Sequence based prediction of RBP binding in Kcnq1ot1}
Within the human transcriptome, we know of at least one lncRNA that silences gene expression in \emph{cis} through a PRC-mediated mechanism similar to \emph{XIST}, \emph{KCNQ1OT1}. The mouse \emph{Kcnq1ot1} has been shown to silence megabase scale regions of chromosome 7 and human \emph{KCNQ1OT1} silences a large region of chromosome 11. Unlike \emph{XIST}, the sequence of \emph{KCNQ1OT1} is not predominantly comprised of tandem repeat domains (Figure \ref{fig:kcndot}), barring a region in the 3' region of the sequence. Furthermore, eCLIP data for RBPs known to be essential for \emph{XIST}'s function show that the predominantly bind in the 5' half of \emph{KCNQ1OT1}'s sequence (Figure \ref{fig:kcnproteins}). 

\begin{figure}[h!]
\centering
\includegraphics[width=.4\textwidth]{images/kcnd.pdf}
\caption{Proteins selectively enriched within tandem repeats of \emph{XIST}. A) $\log_2$ ratio of read density for Re}
\label{fig:kcndot}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{images/kcnproteins.pdf}
\caption{Proteins selectively enriched within tandem repeats of \emph{XIST}. A) $\log_2$ ratio of read density for Re}
\label{fig:kcnproteins}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=.95\textwidth]{images/kcneclip.pdf}
\caption{Proteins selectively enriched within tandem repeats of \emph{XIST}. A) $\log_2$ ratio of read density for Re}
\label{fig:kcnproteins}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Transcriptome-wide RBP Prediction}
Many of the proteins that bind to \emph{XIST} and \emph{KCNQ1OT1} bind throughout the transcriptome. Furthermore, \emph{XIST} clusters with thousands of transcripts in the human genome based on $k$-mer content \cite{Kirk2018FunctionalContent}. Therefore, we hypothesized that \emph{XIST}-like sub-sequences may be found throughout the transcriptome, both in lncRNAs as well as in pre-mRNAs. To test this hypothesis, we trained 4 HMMs on the A-,B-,D-, and E-repeats of human \emph{XIST} as in section 3.2.6 and used \emph{hmmSEEKR} to scan the set of all unspliced coding and non-coding transcripts in the human genome. 

We found that our HMM significantly out-performed a randomized HMM parse for each of the proteins we tested. 

\section{Discussion}
\lipsum[1-2]
\section{Methods}
\subsection{Algorithms}
The functions that drive much of analysis in the \emph{hmmSEEKR} package are found in the corefunctions.py file. These include the algorithms behind HMM analysis including the forward, backward, Viterbi, and Baum-Welch algorithm implementations. In addition, several functions crucial for parsing the supplied DNA sequences and outputting in human readable format are found within this file.

\subsubsection{$k$-mer counting}
$k$-mer counting is at the heart of both SEEKR and hmmSEEKR. This calculation is one of the simplest, but potentially most time consuming portions of the analysis. This is largely because there is no way around reading along the supplied string(s), and counting every $k$-mer as encountered. To speed $k$-mer counting up, \textbf{Algorithm \ref{alg:kmercounts}} has been implented using the cython package within Python, which is a library that allows for C-like implementation of python code.

The pseudo-code in Algorithm \ref{alg:kmercounts} is implemented in kmers.py and kmers.pyx and takes a FASTA file as input. A dictionary, or hash map, with $k$-mers as keys and their counts as values is created, and as each $k$-mer is encountered within the sequence, its value is incremented by one.

\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\KwResult{Python dictionary of $k$-mer counts}
 read in fasta file\;
 strFasta $\leftarrow$ concactenate fasta strings with delimiter character\;
 intL $\leftarrow$ calculate total length excluding delimiter\;
 dictKmerMap $\leftarrow$ initialize dictionary of $k$-mers with initial counts of 1\;
 \For{$k$-mer in sequence}{
  \eIf{$k$-mer in dictKmerMap }{
   dictKmerMap[$k$-mer]$\leftarrow$increment $k$-mer by 1
   }{
   do not increment\;
  }
 }
 Save binary file of python dictionary containing counts
 \caption{Counting $k$-mers from supplied sequences}
 \label{alg:kmercounts}
\end{algorithm}

\subsubsection{Ambiguous Nucleotides}
Transcripts containing regions of ambiguous sequence are of concern when building the viterbi parse, as ambiguous $k$-mers are not included in the emission distributions of the hidden state. Therefore only sequences of $k$-mers containing A,T,C, or G are allowed. To adjust for any ambiguous positions within the sequence, \textbf{Algorithm \ref{alg:ambigIdx}} stores two lists of indices. The first stores the position, or index, of each allowable $k$-mer, whereas the second stores the position of any $k$-mer containining an 'N' within it. If there are any N's within the sequence, list of $k$-mers passed into the Viterbi algorithm are split into distinct sub-sequences, because the regions flanking the ambiguous nucleotides are not adjacent to each other. Finally, when mapping the viterbi parse back to the original sequence, the two lists of indices generated above are used to merge the results back to their original order.

An example would best illustrate what this portion of the code is trying to achieve. For simplicity, let $k=1$, and our observation $X$ be: 
$$X= A,A,N,N,T,T$$
Counting indices from 1, the allowable $k$-mer indices in $X$ are:
$$\{1,2,5,6\}$$
The ambiguous $k$-mer indices are:
$$\{3,4\}$$
As the sequence $X$ is fragmented by two Ns at position 3 and 4, the flanking sequences are passed to the Viterbi algorithm to be parsed separately:
$$\{\{A,A\},\{T,T\}\}$$
Hypothetical merged results following Viterbi parse
$$\{+,+,N,N,-,-\}$$
\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\KwResult{Sequence of unambiguous $k$-mers}
\emph{listO}$\leftarrow$initialize empty list to contain $k$-mers\;
\emph{listIdx}$\leftarrow$initialize empty list to contain $k$-mer indices\;
\emph{listAmbigIdx}$\leftarrow$initialize empty list to contain ambig $k$-mer locations\;
\emph{listKmers}$\leftarrow$list of $k$-mers that can be constructed from ATCG\;
\emph{intIndex}$\leftarrow$0\;
 \For{k-mer \textbf{in} sequence}{
 \eIf{k-mer \textbf{in} listKmers}{
 Append \emph{$k$-mer} to \emph{listO}\;
 Append \emph{intIndex} to \emph{listIdx}\;}{
 Append \emph{intIndex} to \emph{listAmbigIdx}\;
 }
 \emph{intIndex}$+1$\;
 }
 \textbf{return} \emph{listO} \emph{listIdx} \emph{listAmbigIdx}\;
 \caption{Generate unambiguous observed sequence}
 \label{alg:ambigIdx}
\end{algorithm}

\subsubsection{Identifying hits from the Viterbi parse}
Within \emph{hmmSEEKR} a Viterbi parse is a sequence of state labels that have been assigned to each $k$-mer from a DNA sequence, \emph{e.g.} if the DNA sequence was ``AAAACCCC", and $k=4$, then the $k$-mer sequence is $\{AAAA,AAAC,AACC,ACCC,CCCC\}$, and a hypothetical Viterbi parse would be a list such as $\{-,-,-,+,+\}$, where ``$-$" represents the null state, and ``$+$" represents the query state. \textbf{Algorithm \ref{alg:groupHMM}} takes the list $\{-,-,-,+,+\}$ and converts it into a grouped list, \emph{e.g.} $\{\{-,-,-\},\{+,+\}\}$. This also exactly defines what term a hit within our HMM, consecutively labeled ``$+$" regions with a sequence from the viterbi parse.
\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\KwResult{Viterbi backtrack grouped into consecutive states}
\emph{condition}$\leftarrow$Swap condition, \emph{i.e.} '-' encountered\;
\emph{Key}$\leftarrow$Stores a FLAG that switches value when condition is met\;
\emph{previousNuc}$\leftarrow$Previous nucleotide\;
\For{nucleotide in sequence}{
\emph{currNucBool}$\leftarrow$TRUE/FALSE is \emph{nucleotide} in \emph{condition}\;
\emph{prevNucBool}$\leftarrow$TRUE/FALSE is \emph{previousNuc} in \emph{condition}\;
\eIf{currNucBool$\neq$prevNucBool}{swap \emph{Key}\;}{\emph{Key}\;}
\emph{previousNuc}$\leftarrow$nucleotide\;
}
 \caption{Group HMM hits}
 \label{alg:groupHMM}
\end{algorithm}

\subsubsection{HMM Score}
The score for the HMM is calculated for each ``hit" within the transcript, as defined in the prior section. We assume that the set of state labels from the Viterbi algorithm are now known. This allows, for a hit $X$ of length $L$, the likelihood that $X$ was emitted from the query state over its entire length. 

$$P(X=\{x_1,\dots,x_L\} | Y = \{+,\dots,+\}) = \prod_{i=1}^L{p(x_i|+)}$$

This likelihood can be compared to the likelihood of $X$ had it been emitted from the null state, and a likelihood ratio can be calculated.

$$S' = \frac{\prod_{i=1}^L{p(x_i|+)}}{\prod_{i=1}^Lp(x_i|-)}$$

Due to the lengths of most sequences, these calculations are moved to log-space, yielding the formula for the score reported by \emph{hmmSEEKR}. These calculations are implemented in Algorithm \ref{alg:LLR}.

$$S = \sum_{i=1}^L{\left[\log_2 p(x_i|+)- \log_2 p(x_i|-)\right]}$$
\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{arrLLR}$\leftarrow$empty array with an element for each hit\;
\emph{hits}$\leftarrow$list of sequences that HMM called hits\;
\emph{Query}$\leftarrow$Frequency distribution of kmers in query\;
\emph{Null}$\leftarrow$Frequency distribution of kmers in null\;

\For{hit \textbf{in} hits}{
\emph{intLLRQuery}$\leftarrow$ 0\;
\emph{intLLRNull}$\leftarrow$ 0\;
\For{kmer \textbf{in} hit}{
\emph{intLLRQuery}$+\log{P(kmer|Query)}$\;
\emph{intLLRNull}$+ \log{P(kmer|Null)}$\;
}
\emph{arrLLR[hit]}$\leftarrow$\emph{intLLRQuery}-\emph{intLLRNull}\;
}
 \caption{Log-likelihood}
 \label{alg:LLR}
\end{algorithm}

\subsubsection{Viterbi Parse}

The premise behind the Viterbi algorithm is outlined in the introduction. Briefly, we seek to find the sequence of hidden states that maximizes the joint probability function of the HMM defined in equation \ref{eq:jointhmm}, given the observations $X$, the transition matrix $A$, the emission matrix $E$, and the initial probabilities of each hidden state $\pi$. The viterbi algorithm is implemented in \emph{hmmSEEKR} as in \textbf{Algorithm \ref{alg:viterbi}}.

At each step in the Viterbi algorithm, the state transition that maximizes the probability at time $t$ is recorded, for each state. That is, at time $t$, the ``+" state has a most liekly transition from either ``+'' or ``-" from time $t-1$, and the ``-" state at time $t$ has a most likely transition from either ``+" or ``-" from time $t-1$ in the sequence. To identify which was the most likely, a backtrace must be calculated after all these calculations have been made, and this is implemented in \textbf{Algorithm \ref{alg:backtrack}}.

\begin{algorithm}[H]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{O}$\leftarrow$Sequence of kmers\;
\emph{E}$\leftarrow$Emission matrix\;
\emph{A}$\leftarrow$Transition matrix\;
\emph{$\pi$}$\leftarrow$Starting probability of each state\;
\emph{states}$\leftarrow$ List of states \emph{i.e.} [-,+]\;
\emph{dictViterbi}$\leftarrow$List of dicts with cumulative probability at each time $t$ for each state\;
\;
\emph{dictViterbi[1][query]} means the cumulative probability at time $t=1$ at state=query\;\;
\emph{dictMaxState}$\leftarrow$List of dicts containing the state at time $t-1$ that maximized transition to each state at time $t$\;
\;
\;
Initial probabilities at each state at time $t=1$\;
\For{state \textbf{in} states}{
\emph{dictViterbi[1][state]}$\leftarrow \log{P(state|\pi)}+\log{P(O_1|state)}$
}
\;

Calculate probability of being in each state at time $t$ and find which state transition maximizes this probability\;
\For{t \textbf{in} range(2,length of O)}{\;
Add new dictionary element to dictViterbi and dictMaxState
\;
\For{state \textbf{in} states}{\;
selectedState$\leftarrow$ start with the first state to calculate prob\;
currMaxProb$\leftarrow \log P(state|selectedState)+dictViterbi[t-1][selectedState]$\;
\;
\For{checkState in remaining states}{\;
tempProb$\leftarrow \log P(state|checkState)+dictViterbi[t-1][checkState]$\;\;
\eIf{tempProb$>$currMaxProb}{set currMaxProb to tempProb\;}{keep currMaxProb the same\;selectedState$\leftarrow$checkState\;}
}
currMaxProb$\leftarrow$currMaxProb$+\log P(O_t|state)$\;
\emph{dictViterbi[t][state]}$\leftarrow$currMaxProb\;
\emph{dictMaxState[t][state]}$\leftarrow$selectedState\;
}
}
 \caption{Viterbi parse}
 \label{alg:viterbi}
\end{algorithm}

\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined

\emph{backtrack}$\leftarrow$empty list to append maximizing states to\;
\emph{dictViterbi}$\leftarrow$List of dicts with cumulative probability at each time $t$ for each state\;
\;
\emph{dictViterbi[1][query]} means the cumulative probability at time $t=1$ at state=query\;\;
\emph{dictMaxState}$\leftarrow$List of dicts containing the state at time $t-1$ that maximized transition to each state at time $t$\;
\;
\;
maxProbability$\leftarrow$largest probability at time $t=T$\;
maxState$\leftarrow$state associated with maxProbability\;
maxPriorState$\leftarrow$dictMaxState[T][maxState], get state that transitioned to maxState at $t=T-1$\;
\;
Append maxPriorState to backtrack\;

\For{t=T-2 to t=1}{
Append dictMaxState[t+1][maxPriorState] to backtrack\;
maxPriorState$\leftarrow$dictMaxState[t+1][maxPriorState]
}

\textbf{return} backtrack with order flipped to start at $t=1$

 \caption{Backtrack viterbi}
 \label{alg:backtrack}
\end{algorithm}
\subsubsection{Baum-Welch Updating}

\begin{algorithm}[H]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{X}$\leftarrow$Sequence of kmers\;
\emph{Y}$\leftarrow$Sequence of hidden states\;
\emph{E}$\leftarrow$Emission matrix\;
\emph{A}$\leftarrow$Transition matrix\;
\emph{$\pi$}$\leftarrow$Starting probability of each state\;
\emph{states}$\leftarrow$ List of states \emph{i.e.} [-,+]\;
\;

$\alpha(t)\leftarrow$forward probabilities\;
$\beta(t)\leftarrow$backward probabilities\;\;

$\gamma_{i}(t)\leftarrow \frac{P(Y_t=i,X|A,E,\pi)}{P(X_t)}$ ; probability of being in state i at time t\;
$\epsilon_{ij}(t)\leftarrow \frac{P(Y_t=i,Y_{t+1}=j,X|A,E,\pi)}{P(X_t)}$ ; probability of being in state i at time t and state j at time t+1\;
\;

\For{each time "t" in sequence X}{
\For{state "i" in states}{
$\log P(X_t)\leftarrow \textrm{logsumexp}\left(\alpha_i(t)+\beta_i(t)\right)$\;
$\gamma_i(t)\leftarrow \alpha_i(t)+\beta_i(t) - \log P(X_t)$\;
\For{state "j" in state}{
$A_{ij}(t+1)\leftarrow \log P(Y_{t+1}=j|Y_t=i)$\;
$E_j(t+1)\leftarrow \log P(X_{t+1}|Y_{t+1}=j)$\;
numerator$\leftarrow \alpha_i(t)+A_{ij}(t) + \beta_j(t+1)+E_j(t+1)$\;
denominator$\leftarrow \log P(X_t)$\;\;
$\epsilon_{ij}(t)\leftarrow$numerator-denominator\;
}
}
Append new entry to $\gamma$ list of dicts\;
Append new entry to $\epsilon$ list of dicts\;
}

Sum $\gamma$ and $\epsilon$ over all values of $T$ for all states $i$ and $j$. The sum below represents the expected number of transitions from state $i$ to state $j$ over the sequence. 
$$A^*_{ij} = \frac{\sum_t{\epsilon_{ij}(t)}}{\sum_t{\gamma_{i}(t)}} $$\;

Do not update emission matrix $E$.\;\;
\textbf{return} $A^*$\;

 \caption{Baum Welch Parameter Update}
 \label{bw}
\end{algorithm}

\begin{algorithm}[H]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{X}$\leftarrow$Sequence of kmers\;
\emph{Y}$\leftarrow$Sequence of hidden states\;
\emph{E}$\leftarrow$Emission matrix\;
\emph{A}$\leftarrow$Transition matrix\;
\emph{$\pi$}$\leftarrow$Starting probability of each state\;
\emph{states}$\leftarrow$ List of states \emph{i.e.} [-,+]\;
\;

$\alpha_i(t)\leftarrow$forward probabilities, indexed by time $t$ and state $i$ \emph{i.e.} the probability of being in state $i$ at time $t$\;
$T\leftarrow$length of X\;\; 

Initialize probabilities at $t=1$\;
$\alpha_i(t=1)\leftarrow \pi(i) + \log P(X_t |Y_t = i)$\;\;
\For{t in T}{
Append new entry to $\alpha$\;
\For{state "i" in states}{
currP$\leftarrow$\{+:0,-:0\} : dict with entry for each state, indexed by state name\;
\For{state "j" in states}{
$A_{ij}(t)\leftarrow P(Y_t=i|Y_{t-1}=j)$\;
currP[j]$\leftarrow \alpha_j(t-1) + A_{ij}(t)$\;
}
$\alpha_i(t)\leftarrow \textrm{logsumexp}\left(currP\right)+\log P(X_t|Y_t=i)$\;
}
}
 \caption{Forward Algorithm}
 \label{fwd}
\end{algorithm}

\begin{algorithm}[H]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{X}$\leftarrow$Sequence of kmers\;
\emph{Y}$\leftarrow$Sequence of hidden states\;
\emph{E}$\leftarrow$Emission matrix\;
\emph{A}$\leftarrow$Transition matrix\;
\emph{$\pi$}$\leftarrow$Starting probability of each state\;
\emph{states}$\leftarrow$ List of states \emph{i.e.} [-,+]\;
\;

$\beta_i(t)\leftarrow$backward probabilities, indexed by time $t$ and state $i$ \emph{i.e.} the probability of being in state $i$ at time $t$\;
$T\leftarrow$length of X\;\; 

Initialize probabilities at $t=T$\;
$\beta_i(t=T)\leftarrow 0$ ; probability of 1 in log space\;\;
\For{T to t=1}{
Append new entry to $\beta$\;
\For{state "i" in states}{
currP$\leftarrow$\{+:0,-:0\} : dict with entry for each state, indexed by state name\;
\For{state "j" in states}{
$A_{ij}(t)\leftarrow P(Y_{t+1}=j|Y_{t}=i)$\;
$E_j(t+1)\leftarrow P(X_{t+1}|Y_{t+1}=j)$\;
currP[j]$\leftarrow \beta_j(t+1) + A_{ij}(t) + E_j(t+1)$\;
}
$\beta_i(t)\leftarrow \textrm{logsumexp}\left(currP\right)$\;
}
}

\textbf{return} $\beta$ inverted s.t. $1\rightarrow T$ rather than $T \rightarrow 1$\;
 \caption{Backward Algorithm}
 \label{bwd}
\end{algorithm}
\pagebreak

\subsection{Python implementation}
\begin{table}[t!]
\centering
 \begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
 \hline
 Program & Input & Purpose\\
 \hline\hline
 kmers.py & fasta file & Count $k$-mers within fasta files provided \\ 
 \hline
 train.py & counts files from kmers.py for query and null & Generate matrices that define the HMM\\
 \hline
 mSEEKR.py & fasta file for sequence(s) of interest, path to output of train.py & retrieve the viterbi parse of the sequence of interest to identify potential functional domains \\
 \hline
bw.py & HMM training sequences, initial parameterizations & provide MLE of transition parameters \\
 \hline
 
\end{tabular}
\caption{Individual programs within the \emph{hmmSEEKR} package.}
\label{programs}
\end{table}
\emph{hmmSEEKR} has been implemented as a python package in order to rapidly train HMMs and scan sequences of interested for potential function domains. Given the general algorithms defined above, which roughly correspond to individual functions within the \emph{hmmSEEKR} package, a thorough guide is provided for how the algorithms have been implemented. 

\subsubsection{Installation}
Table \ref{programs} provides an overview of the different python programs within the \emph{hmmSEEKR} package. The first step is to ensure that Python3.6 or greater is installed, as well as the SEEKR package and hmmSEEKR repository which can be retrieved entering the following commands:

\begin{verbatim}
> pip install seekr
> git clone https://github.com/spragud2/mSEEKR
> cd mSEEKR/
> python setup.py build_ext --inplace
\end{verbatim}

\subsubsection{$k$-mer frequencies}
To count $k$-mers for a set of sequences, a single fasta file containing any number of sequences is required. The program counts the $k$-mers in all sequences, and then calculates the average over all of them to provide a single distribution of $k$-mer frequencies. Additionally the program is capable of multi-processing over different values of $k$, such that 2-,3-,4-,$\dots$-mers can be calculated simultaneously. 
\begin{table}[h]
\centering
 \begin{tabular}{|l l|}
 \hline
 Parameter & Function\\
 \hline
 --fasta & Path to fasta file \\
 --name & Output file name \\
 --dir & Output directory \\
 -k & Comma delimited list of values of $k$ \emph{e.g.} 2,3,4,5\\
 -a & Alphabet to use \emph{e.g.} ATCG\\
 -n & Number of processors to use\\
 \hline
 
\end{tabular}
\caption{Parameters for kmers.py}
\label{tab:kmerparams}
\end{table}

The program counts $k$-mers as defined in Algorithm \ref{alg:kmercounts}. The following python code reads in the parameters defined above. The provided fasta file is converted into a singular string with a delimiter character \$, and the total length of the string excluding the delimiter is calculated. Finally, the script kmers.pyx, which contains the implementation of Algorithm \ref{alg:kmercounts}, is spooled onto the number of processors passed, and the results are compiled into a dictionary. 

\begin{verbatim}
> python kmers.py --fasta ./fastaFiles/gencode.vM17.lncRNA_transcripts.fa -k 2,3,4 --name mm10Trscpts -n 3
\end{verbatim}

The results are then saved into a binary file containing a python dictionary containing $k$-mer frequencies for each value of $k$ specified in the arguments. 
\lstinputlisting[language=Python]{code/kmersMain.py}
\subsubsection{HMM Creation}
To create the HMM files necessary for running the mSEEKR.py program, the train.py defines the matrices $A,E,\pi$ that are defined in Algorithms \ref{viterbi},\ref{fwd},\ref{bwd} and saves them in a single binary file. The emission probabilities are equivalent to the $k$-mer frequencies calculated from kmers.py. Due to restrictions of available training data for our own experiments, the user can manually provide the self-transition parameters for the query and null hideen states.

If training sequences are available, the --bw flag can be passed to run the Baum-Welch algorithm to obtain an MLE for the transition parameters. If the --bw flag is passed, the --iter argument must be passed. The results from the --bw are then passed directly into the main train.py program. To better check the results of a Baum-Welch operation it is recommended to run the bw.py program separately with a variety of initial parameterizations. This allows for inspection of whether local minima or a potential global maximum have been reached. The results from the previous experiment can then be manually passed to train.py --qT and --nT arguments.
\begin{table}[h]
\centering
 \begin{tabular}{|l l|}
 \hline
 Parameter & Function\\
 \hline
 --query & Path to kmer counts of query \\
 --null & Path to kmer counts of null \\
 --qT & Query$\rightarrow$Query transition probability \\
 --nT & Null$\rightarrow$Nulltransition probability \\
 --qPrefix & Name of query for file path \\
 --nPrefix & Name of null for file path\\
 --dir & Directory to put HMM model \\
 -k & Comma delimited list of values of $k$ \emph{e.g.} 2,3,4,5\\
 -a & Alphabet to use \emph{e.g.} ATCG\\
 \hline
 
\end{tabular}
\caption{train.py parameters}
\label{tab:trainparams}
\end{table}
\linebreak

\emph{hmmSEEKR} creates directories for the save files in a predefined fashion to ensure proper retrieval of the correct matrices when running mSEEKR.py. THe following code checks to see if the directory specified in --dir exists, and if not, creates the directory. 

\begin{verbatim}
> python train.py --query ./counts/mouseA.skr --null ./counts/mm10Trscpts.skr -k 2,3,4 --qPrefix mouseA --nPrefix mm10Trscpts --qT .9999 --nT .9999 --dir ./markovModels/
\end{verbatim}
\lstinputlisting[language=Python]{code/trainDir.py}
The script then loads the $k$-mer counts specified in --query and --null and passes and begins looping through the values of $k$ specified in the -k argument. The following code loops through each value of $k$, loads the $k$-mer frequencies, and creates the matrices $A,E,\pi$ in the form of a python dictionary. The 2-state dimensions of this HMM are hard-coded into the script. 

\lstinputlisting[language=Python]{code/trainSave.py}
The resulting python dictionaries are then saved into a binary file in a dictionary within the directory matching the following pattern:

\begin{verbatim}
    --dir/--qPrefix_--nPrefix/-k/hmm.mkv
\end{verbatim}
\subsection{Viterbi Parsing}
The primary script within the $hmmSEEKR$ is mSEEKR.py. This script takes the matrices $A,E,\pi$ created from train.py and scans a sequence, or set of sequences, and calculates the most likely parse, or Viterbi path, through the sequence. Therefore a sequence, \emph{e.g.} ATCG, is converted into an equal length string of state lebels, \emph{e.g.} - - + -. The program then extracts consecutively + (query) labeled nucleotides, and reports each such region as a \emph{hit}. 

\begin{verbatim}
> python mSEEKR.py --db ./fastaFiles/mm10kcn.fa -n 8 --prefix test --model ./markovModels/mouseA_mm10Trscpts -k 3 --fasta
\end{verbatim}

\begin{table}[h]
\centering
 \begin{tabular}{|l l|}
 \hline
 Parameter & Function\\
 \hline
 --model & Path to the HMM model created in train.py \\
 -k & Length of short motif (k-mer) \\
 --db & Sequence database to use, currently accepts FASTA format\\
 --prefix & Output file name prefix \\
 -a & Alphabet to use \emph{e.g.} ATCG \\
 -n & Number of processors to use (default=1)\\
 --fasta & Include sequence in output \\
 \hline
 
\end{tabular}
\caption{mSEEKR.py parameters}
\label{tab:viterbiparams}
\end{table}

The following code section from mSEEKR.py loads in the specified HMM model from the --model argument, and ensures that the path has been provided in the correct format, \emph{i.e.} ending with a ``/". 
\lstinputlisting[language=Python]{code/mSEEKRLoad.py}

The sequences to be parsed are then spooled onto the number of threads provided in the -n argument, and that is executed by this code:

\lstinputlisting[language=Python]{code/mSEEKRpool.py}

Each sequence is then prepared for input into the Viterbi algorith. First, the sequences are passed to the function \texttt{kmersWithAmbigIndex}, which implements the pseudo-code outlined in Algorithm \ref{alg:ambigIdx}. The next step is the calculation of the Viterbi parse, as outlined in Algorithm \ref{alg:viterbi}. 

\lstinputlisting[language=Python]{code/mSEEKRcalc.py}

The parsed sequences are then combined with any ambiguous sequence regions from the original supplied sequence such that we perfectly reconstruct the original sequence, but with hidden state labels instead of nucleotides. 

Finally the regions that have been designated as ``hits" are compiled in a dataframe within the \texttt{formatHits} and \texttt{hitOutput} functions. Additionally, this is where the HMM score is calculated as in Algorithm \ref{alg:LLR}. 

\subsection{Xist Enriched Proteins}

ENCODE eCLIP Experiments (non-controls) in K562 cells bigWig Files
\begin{verbatim}
xargs -L 1 curl -O -L < encodefileurls.txt
for i in *bigWig; do ./bigWigToBedGraph $i ${i}.bedGraph;done
\end{verbatim}



Calculate ratio of average eCLIP signal within Xist Repeats to the average signal outside repeats. 

We defined $X$ as a set of size $L = 19275$, where each entry corresponds to a basepair in \emph{XIST}. Each nucleotide within this set, $x_i$, was given a value depending on whether or not eCLIP signal was present from the bigWig files retrieved from ENCODE. If there was no signal, a value of 0 was given to $x_i$, otherwise the read density from the bigWig file was assigned to $x_i$.

We calculated the eCLIP signal enrichment of each protein within each \emph{XIST} repeat by using the transcript coordinates of each repeat in \emph{XIST} to define a set $I$, which contains all individual nucleotide positions within a given \emph{XIST} repeat. Set $I^c$ is the complement of $I$, relative to $X$,  and contains all basepairs in \emph{XIST} that are not in $I$. 

If $N$ is the size of $I$, \emph{i.e.} the size of the given repeat in \emph{XIST}, then the ratio of average signal $R$ is:

$$R = \frac{L-N}{N}\frac{\sum_{i\in{I}}{X_i}}{{\sum_{j\in{I^c}}{X_j}}}$$

\subsection{\emph{Rsx} HMM Analysis}

Human \emph{XIST} repeats A,B,D,E were used as queries within 4 separate \emph{hmmSEEKR} models. For the null model, human unspliced lncRNAs (GENCODE V26) were used. A model was trained for each value of $k \in \{2,3,4,5,6\}$ and for the transition parameters all combinations of the following values were tested, $\alpha,\beta \in \{.5, .75, .9, .99, .999, .9999$\}. The koala \emph{Rsx} transcript was then scanned using each of these HMMs. For measuring performance of the HMM, precision was defined as the number of nucleotides from the HMM Viterbi parse that fell within the correct \emph{Rsx} repeat as found in Sprague et al. 2019. Recall was defined as the percentage of each \emph{Rsx} repeat recovered from the associated \emph{XIST} trained HMM. The $F_1$ score was then calculated as 

\begin{equation}
    F_1 = \left(\frac{2}{\texttt{precision}^{-1}+\texttt{recall}{^-1}}\right)
\end{equation}

And the set of parameters for each \emph{XIST} query with the highest $F_1$ score was chosen (Table \ref{tbl:rsxparams}.

\begin{table}[h]
\centering
\begin{center}
 \begin{tabular}{|l| l| l | l |} 
 \hline
 Query & k & $\alpha$ & $\beta$ \\
 \hline
 Repeat A & 4 & .9999 & .9999 \\ 
 \hline
 Repeat B & 4 & .9999 & .9999\\
 \hline
 Repeat D & 2 & .9999 & .9999\\
 \hline
 Repeat E & 4 & .9999 & .9999\\
 \hline
\end{tabular}
\end{center}
\caption{Best values of k, the $Q\rightarrow Q$ parameter $\alpha$, and $N\rightarrow N$ parameter $\beta$ as determined through a grid search based approach to identify sequences with the highest SEEKR correlation to the query.}
\label{tbl:rsxparams}
\end{table}

\subsection{Transcriptome Search}
The goal of this experiment is to determine if mSEEKR is significantly outperforming random with respect to overlap of eCLIP hits for proteins that are significantly enriched within the XIST locus. These proteins and their respective queries are:

1. Repeat A - SRSF1 and RBM15
2. Repeat B - HNRNPK
3. Repeat D - HNRNPK
4. Repeat E - TIA1, MATR3, PTBP1

These eCLIP files were sourced from ENCODE, with the following experiment names:

1. SRSF1 - ENCFF696TJG
2. RBM15 - ENCFF739LLZ
3. HNRNPK - ENCFF894NKS
4. TIA1 - ENCFF080VML
5. MATR3 - ENCFF162SAS
6. PTBP1 - ENCFF765BPN


The following RNAseq data was used to identify transcripts with high association with chromatin and unspliced expression.

1. K562 Chromatin Association RNAseq - ENCFF957VGD


The following programs were used to perform the experiment

1. mSEEKR/v1.0.8
2. RSEM/1.2.31
3. deeptools/3.0.0
4. subread/2.0.0

1. Create a fasta file with transcript isoforms of interest. For this analysis, a single spliced isoform (primary) and the unspliced transcript
  - make note of the genome used to make this file
  - in this example, let this be v26_combo.fa
2. In the directory with the fasta file from (1), run the following commands

```module add bowtie/1.2.2
module add rsem/1.2.31
module add samtools
module add deeptools/2.5.2
sbatch --mem 50g -t 5:00:00 -N 1 -n 24 --wrap='bowtie-build --threads 24 v26_combo.fa hgT'
sbatch --mem 50g --wrap='rsem-prepare-reference . hgT'
```

3. Run RSEM with paired end reads:

sbatch -N 1 --mem 50g -t 5:00:00 -J rsemChr -n 24 --wrap='rsem-calculate-expression -p 24 --paired-end READS1.fastq READS2.fastq  hgT EXPNAME'


- With single reads, just provide a single fasta file and remove the --paired-end arg

- EXPNAME - file prefix for output


4. In a new directory, wget the genome file for the corresponding genome that was used in (1)
  sbatch --wrap='wget URL'

5. Using the same version of bowtie as in (2), build the index for this genome:

sbatch --mem 50g -t 5:00:00 -N 1 -n 24 --wrap='bowtie-build --threads 24 GRCh38.p14.genome.fa hg38'

6. Align the original fastq reads using bowtie

sbatch -N 1 -n 24 -t 5:00:00 --mem 50g --wrap='bowtie --threads 24 -S ./hg38 path/to/READS1.fastq  path/to/READS2.fastq aln.out'

7. Process with samtools and deeptools

```sbatch --wrap='for f in *out; do samtools view -S -b $f > ${f}.bam; done'
sbatch --wrap='for f in *bam; do samtools sort  $f > ${f}sorted.bam; done'
sbatch --wrap='for f in *sorted.bam; do samtools index $f; done'
```
Get forward and reverse bedgraphs

sbatch -J FORWARD -t 50:00:00 -N 1 -n 24 --wrap='bamCoverage --filterRNAstrand forward -bs 100 -p 24 -b aln.outsorted.bam -of bedgraph -o alnFORWARD.bedGraph'

sbatch -J REVERSE -t 50:00:00 -N 1 -n 24 --wrap='bamCoverage --filterRNAstrand reverse -bs 100 -p 24 -b aln.outsorted.bam -of bedgraph -o alnREVERSE.bedGraph'

This portion of the protocol takes the SAF files and counts the number of reads from the BAM file that overlap HMM hits, using the following commands:

```
featureCounts -a hmmE.saf -o tia1E  tia1.bam -s 2 -F SAF -p -f
featureCounts -a randomE.saf -i tia1RandomE tia1.bam -s 2 -F SAF -p -f 
```