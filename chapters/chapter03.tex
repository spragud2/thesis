\section{Introduction}
\emph{XIST} and \emph{Rsx} provide compelling examples of how the sequence-to-function relationship in lncRNAs may be modular in nature \cite{Brockdorff2018LocalNcRNA, Sprague2019NonlinearDomains,Pintacuda2017HnRNPKSilencing,Wang2017TargetingGuanines,Zhao2008PolycombChromosome}. These two transcripts demonstrate clear sequence regions, defined by the boundaries of their tandem repeat domains \cite{Brown10TheNucleus.,Brockdorff10TheNucleus.,Sprague2019NonlinearDomains,Grant2012RsxInactivation}, that are conserved and are clearly functional as demonstrated by the binding of RNA binding proteins to their transcripts \cite{Hoki2009AMouse,Moindrot2015ASilencing,Sunwoo2017RepeatCIZ1,Nesterova2001CharacterizationSequence,Royce-Tolland2010TheInactivation,Brown10TheNucleus.,Brockdorff10TheNucleus.}. Our model of lncRNA function, specifically the bag-of-words model that the order of short motifs in a sequence is less important than their overall density \cite{Kirk2018FunctionalContent}, implicitly means that a tandem repeated sequence may not be essential for recruitment of these RNA binding proteins \cite{Sprague2019NonlinearDomains}.

It is possible to show that the $k$-mer content of a sequence can be perfectly preserved while destroying the tandomly repeated nature of a sequence (Fig \ref{fig:shufD} A-C). This can be done by building a graph whose nodes are $(k-1)$-mers and whose edges are connected such that a sequence is reconstructed with the exact same $k$-mer frequencies as the original sequence \cite{Jiang2008UShuffle:Counts}. Furthermore, if we examine lncRNA sequences in mouse with very high SEEKR correlation to HXD it is clear that not all these sequences contain substantial tandemly repeated sequence as in \emph{XIST} (Fig \ref{fig:shufD} D-H). 
\begin{figure}[h!]
\centering
\includegraphics[width=.85\textwidth]{shufD.png}
\caption[Shuffling $k$-mers abrogates tandem repeat without changing motif content]{Dot plot alignments and SEEKR-defined similarity between human Xist Repeat D, shuffled Repeat D, and the top 5 mouse lncRNAs that are most similar to Repeat D. Repeat D was shuffled using $\mu$Shuffle and preserving $k$-mer content at $k$ = 4 (Jiang et al., 2008). Dotplots were generated using a window size 20 nucleotides and a threshold of 50\% identity.}
\label{fig:shufD}
\end{figure}

Therefore, a challenge in understanding lncRNA function is identifying where functional modules may be located within a sequence \cite{Pang2006RapidFunction,Hezroni2015PrinciplesSpecies,Johnson2014TheRNAs}. Clearly, the answer is not solely exonic sequence as in a protein coding reading frame. Within \emph{XIST} these sequence regions are sub-sequences within the spliced transcript \cite{Brockdorff10TheNucleus.,Brown10TheNucleus.,Brockdorff2018LocalNcRNA,Sprague2019NonlinearDomains,Pintacuda2017HnRNPKSilencing,Hoki2009AMouse,Sunwoo2017RepeatCIZ1}. Compounding the issue is that as far as is known, no specific boundary motifs (\emph{e.g.}, splice junction boundaries) are required. Potential splice junctions in genes are relatively easy to identify based off a combination of sequence content, and the presence of conserved 5' and 3' splice site motifs that demarcate the beginning and end of an intron, respectively \cite{Burge1997PredictionDNA}. 

To model this hypothesized sequence structure, we have developed an HMM that is designed to identify sub-sequences within a larger transcript that contain regions of elevated $k$-mer content to a known functional domain. Given how little is known in the field about the sequence-to-function relationship in lncRNAs, we use the archetypal lncRNA \emph{XIST} and its known functional sub-sequences (A-,B-,D-,E- repeats) as our model. 

We demonstrate here the first computational tool capable of detecting sequence similarity between functionally related but non-homologous  long non-coding RNAs in the human genome. Rather than deriving functional relationships through homology as in traditional sequence alignment \cite{Altschul1990BasicTool,Wheeler2013Nhmmer:HMMs}, we consider and model the problem of functional analogy between non-coding transcripts. lncRNAs are free to mutate at significantly faster rates than mRNAs however that lack of conservation has been shown to not necessarily imply lack of function \cite{Johnsson2014EvolutionaryFunction,Sprague2019NonlinearDomains,Nesterova2001CharacterizationSequence}. The model developed here is capable of identifying sequence features that are much too subtle and jumbled for even the most sensitive alignment algorithms to detect \cite{Wheeler2013Nhmmer:HMMs,Altschul1990BasicTool,Rice2000EMBOSS:Suite}, yet these same features have been shown to strongly predictive of function \cite{Kirk2018FunctionalContent,Sprague2019NonlinearDomains}. 

\section{Results}
\subsection{Model Structure}
Given a transcript sequence of $k$-mers, $X$, of length $L$, and a set of predefined functional features $Y$, the objective is to map each $x \in X$ to a functional state, $y \in Y$ through an HMM. Our underlying biological hypothesis is that non-coding RNAs contain sub-sequences of specific $k$-mer content that are enriched for motifs that bind a specific RBP or subset of RBPs and that these domains are free to start and stop within a larger sequence\cite{Brockdorff2018LocalNcRNA,Hezroni2015PrinciplesSpecies,Pang2006RapidFunction}. The primary purpose of the HMM is to identify the most likely positions within a sequence that these RBP-interacting functional domains begin and end. We therefore defined the first functional feature to be the \emph{query}, which is a categorical distribution of $k$-mer frequencies from training sequences known to have some specific functional role (e.g. tandem repeats of \emph{XIST}). We defined one additional functional feature which comprises a \emph{null} feature. The null state is a categorical distribution of $k$-mer frequencies that represent the average of the transcriptome, or background sequence, which is designed to encapsulate all other sequences that don't match the \emph{query}.  

The two hidden states, \textit{query} and \textit{null} are represented by the symbols ``+'' and ``-'' respectively, throughout the rest of this chapter, as this is how they are coded within the python implementation of \emph{hmmSEEKR}. 

From a probabilistic point of view, the HMM models an RNA sequence by assuming the sequence we observe ($X$) is stochastically sampled from an unknown sequence of hidden states (functional domains) that control the behavior of $X$ \cite{Rabiner1989ARecognition}. Prior HMM based models have primarily modeled the emission of a sequence of individual nucleotides and incorporated high-order interactions through conditional probabilities \cite{Burge1997PredictionDNA,Pachter2002ApplicationsProblems,Henderson1997FindingModel}. 

As an example, if the following sequence were observed, the probability of that sequence occurring could be calculated using varying amounts of prior information. Generally, the more relationships captured within the sequence, the better the sequence can be modeled (Table \ref{tbl:highorder}). 

\begin{table}[h!]
\centering
 \begin{tabular}{|c | c| c |}
 \hline
 \multicolumn{3}{|c|}{Observed Sequence $X = ATCGA$}\\
 \hline
 Markov Order & P(X) & Parameters\\
 \hline\hline
 0 & $P(A)P(T)P(C)P(G)P(A)$ & 4 \\ 
 \hline
 1 & $P(A)P(T|A)P(C|T)P(G|C)P(A|G)$ & 16\\
 \hline
 2 & $P(A)P(T|A)P(C|AT)P(G|TC)P(A|CG)$ & 64 \\
 \hline
 3 & $P(A)P(T|A)P(C|AT)P(G|ATC)P(A|TCG)$ & 256 \\
 \hline
 
\end{tabular}
\caption[Conditional dependencies in a DNA sequence]{Progressively more prior history can be incorporated to model the probability distribution of a sequence, however doing so exponentially increases the number of parameters to be estimated.}
\label{tbl:highorder}
\end{table}


Given the underlying assumptions of SEEKR \cite{Kirk2018FunctionalContent} we chose to model the emissions of $k$-mers directly rather than individual nucleotides conditioned on $k$ previous nucleotides. When constructing a parse of a sequence, this choice better represents the bag-of-words model underyling SEEKR \cite{Kirk2018FunctionalContent} while allowing each emission to be conditionally dependent only on the functional feature emitting it (Figure 3.2).

These two probabilities, \textit{e.g.} $P(ATCGA)$ \textit{vs.} $ P(A|ATCG)$, are very similar to each other, and are related by a normalizing factor. \emph{E.g.}, if $k=5$, then the probability of observing the $5$-mer $ATCGA$ is equivalent to the probability of observing $A$ given the probability of the preceding nucleotides $ATCG$, multiplied by the probability of observing $ATCG$.

$$P(ATCGA) = P(A|ATCG)P(ATCG)$$
This example illustrates another reason that we chose to model the joint probability of a $k$-mer rather than the conditional probability of a nucleotide on the preceding $k$-mer -- the joint probability distribution better models differentials between enriched and depleted $k$-mers in a bag-of-words model, whereas the conditional distribution better models longer-range interactions in the sequence. \textit{E.g.}, the $k$-mer ATCG might be quite rare in the training data, but A preceded by ATCG might have a high probability so long as it is observed, and so within the context of the HMM this sparsity of ATCG would not get modeled as desired in SEEKR.  


The last modeling choice we make is to construct the HMM such that each hidden state $y$ has a non-zero probability of transitioning to any other state, including itself. This is formally known as an ergodic HMM \cite{Rabiner1989ARecognition}. Prior HMM models of DNA sequences are primarily left-right structured \cite{Burge1997PredictionDNA,Pachter2002ApplicationsProblems,Henderson1997FindingModel,Wheeler2013Nhmmer:HMMs}, due to prior knowledge of the structure of a gene, or motif. If a model for a gene were constructed, the transition Intron $\rightarrow$ 5' UTR can be assigned a probability of zero given prior knowledge. Within lncRNAs, we and others hypothesized and shown that protein binding functional domains are modular in nature and therefore the hidden states within our HMM are free to transition in all directions.

\begin{figure}[h]
\centering
\includegraphics[width=.75\textwidth]{model.png}
\caption[Graphical model of \emph{hmmSEEKR}]{Graphical model of HMM SEEKR. The Query (``+") hidden state represents the functional domain to be identified in other lncRNA sequences and the Null (``-") hidden state represents the average $k$-mer frequencies of the transcriptome. The $\alpha$ and $\beta$ parameters represent the self-transition probabilities for the hidden state. }
\label{fig:hmmseekrmodel}
\end{figure}

\subsection{Viterbi Parsing and Scoring}
The primary use for the HMM in \emph{hmmSEEKR} is identifying the most likely parse, $\phi_{max}$ for a given sequence or set of sequences, $X_i \in \{X_1,X_2,\dots,X_n\}$, where each $X_i$ is a sequence of $k$-mers. The regions that we define as ``hits" are all sub-sequences of contiguously query-labeled sequence, \emph{e.g.} for a sequence/parse pair such as:

$$(X_i=ATCGCCCG,\phi_{max}=-,-,+,+,+,+,+,+)$$

The ``hit" in this example as defined in \emph{hmmSEEKR} is the sub-sequence CGCCCG. There may be any number of hits within a sequence, but a hit is always composed of contiguous ``$+$" (query) labeled nucleotides.  The Viterbi path through each sequence was calculated as in algorithms \ref{alg:viterbi}, \ref{alg:backtrack} (Methods), and was implemented in corefunctions.py within the \emph{hmmSEEKR} package. This code can be called as in section 3.4.3 within the mSEEKR.py program.

The score for the HMM, $S$, is calculated for each ``hit” within the transcript. We assume that the set of state labels from the Viterbi algorithm are now known. The score is the log-likelihood of the hit belonging to the query state relative to the null state (Equation \ref{eq:score}; Methods 4.2, HMM Score). In Equation \ref{eq:score}, $x_i$ is an individual $k$-mer within the hit, $L$ is the length of the hit, and the probability distributions $p(x_i|+)$ and $p(x_i|-)$ correspond to the $k$-mer frequency distributions in the emission matrix. 
\begin{equation}
    S = \sum_{i=1}^L{\left[\log_2 p(x_i|+)- \log_2 p(x_i|-)\right]}
\label{eq:score}
\end{equation}

Table \ref{tbl:hmmresults} illustrates the output of mSEEKR.py when scanning mouse \emph{Xist} using a human \emph{XIST} query and parameters $\theta = (k=4,\alpha=.9999,\beta=.9999)$ (Methods 4.1 Transition Parameters and $k$). Here, an HMM was trained on human \emph{XIST} repeat A as the query hidden state, and the null hidden state was trained on the set of all unspliced lncRNAs in  mice (Methods 3.4.3,\cite{Derrien2012TheExpression}), and mouse \emph{Xist} was scanned for matches to the repeat A query. In mouse \emph{Xist}, repeat A is defined as spanning basepairs 292-713 \cite{Brockdorff10TheNucleus.}, and \emph{hmmSEEKR} called basepairs 201-748 as a hit to the repeat A query within mouse Xist (Table \ref{tbl:hmmresults}), yielding 100\% recovery of the original sequence. The score $S$ is displayed in the kmerLLR column in the output. As the original definition for repeat A was formally defined by the presence of the tandem repeat \cite{Brockdorff10TheNucleus.,Brown10TheNucleus.}, and not based on functionality, such as RBP motif enrichment or experimentally determined presence of protein binding, it is significantly more challenging to accurately assess the false positive rate of the HMM (Figure \ref{fig:hnkhumanmouse}).


\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline 
Rank&Start & End   & Length & kmerLLR & seqName                               \\
\hline 
0     & 201   & 748    & 547     & 285.225  & \textgreater{}xist \\
1     & 10295 & 11052  & 757     & 42.734   & \textgreater{}xist \\
2     & 1532  & 1576   & 44      & 30.685  & \textgreater{}xist \\
3     & 11326 & 11450  & 124     & 29.306   & \textgreater{}xist \\
4     & 17941 & 17946  & 5       & -0.968 & \textgreater{}xist\\
\hline 
\end{tabular}
\caption[\emph{mSEEKR.py} output file]{Output of \emph{mSEEKR.py}. Each hit from scanning \emph{XIST} with an HMM trained on the A-repeat of mouse \emph{Xist} is shown and sorted by the \emph{hmmSEEKR} score, ``kmerLLR". The parameter vector $\theta = (k,\alpha,\beta)$ for this HMM is $\theta_A = (4,.9999,.9999)$. }
\label{tbl:hmmresults}
\end{table}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{images/hnkmousehuman.png}
\caption[HNRNPK enrichment in human \emph{XIST} and mouse \emph{Xist}.]{UCSC wiggle- density display of HNRNPK CLIP data (orange) aligned over the mouse (mm9) and human (hg38) Xist genomic loci. Mouse and human clip data are from \cite{Cirillo2016QuantitativeEditor,VanNostrand2016RobusteCLIP}. Arrows denote direction of Xist transcription. Black rectangles indicate genomic locations of mouse and human repeat sequences used in this work.}
\label{fig:hnkhumanmouse}
\end{figure}

As there are no pre-existing annotations for these $k$-mer based sequence features within lncRNAs, it is difficult to define what constitutes a true positive and a true negative. For example, HNRNPK binds large regions of sequence outside the formal definitions of the B- and D-repeats in human \textit{XIST} (Figure \ref{fig:hnkhumanmouse}). To help elucidate what sequence features within \emph{XIST} and other lncRNAs are functional, beyond mere repetitiveness, we turned to protein binding data to identify proteins most associated with core tandem repeat domains within \textit{XIST}. 

\subsection{Xist Associated RBPs}
A crucial feature of the \emph{XIST} transcript are the 4 core repeats that bind unique subsets of RNA binding proteins \cite{Sunwoo2017RepeatCIZ1,Zhao2008PolycombChromosome,Pintacuda2017HnRNPKSilencing,Wang2017TargetingGuanines,Hoki2009AMouse}. Several RBPs have been shown to be crucial for the function of \emph{XIST}, including HNRNPK, RBM15, and several others \cite{Sunwoo2017RepeatCIZ1,Zhao2008PolycombChromosome,Pintacuda2017HnRNPKSilencing,Wang2017TargetingGuanines,Hoki2009AMouse,Chu2015SystematicProteins}. The tandem repeats have been defined based only on their repetitiveness, but Figure \ref{fig:shufD} demonstrates that a non-repetitive sequence can have the same motif content as a repetitive one. Indeed, examination of ENCODE eCLIP data for HNRNPK reveals significant eCLIP binding adjacent to and extending beyond the formally defined B- and D-repeats of \emph{XIST} (Figure \ref{fig:hnkhumanmouse}). Therefore, we hypothesized that the functional elements of a lncRNA such as \emph{XIST} are not defined by repetitiveness but rather by enrichment of protein binding motifs \cite{Kirk2018FunctionalContent,Sprague2019NonlinearDomains,Dominguez2018SequenceProteins,Ray2013ARegulation,Wang2017TargetingGuanines}. To further test \emph{hmmSEEKR}, we sought to identify the proteins most associated with the A,B,D,E-repeats of \emph{XIST} so that we could validate the predictive power of our model against existing protein binding data.
\begin{figure}[h!]
\centering
\includegraphics[width=.8\textwidth]{images/sigproteins.pdf}
\caption[\emph{XIST} tandem repeat enriched proteins]{Proteins selectively enriched within tandem repeats of \emph{XIST}. A) $\log_2$ ratio of read density for Repeat A compared to the remainder of \emph{XIST}. B) $\log_2$ ratio of read density for Repeat B compared to the remainder of \emph{XIST} is shown. C) $\log_2$ ratio of read density for Repeat D compared to the remainder of \emph{XIST}. D) $\log_2$ ratio of read density for Repeat E compared to the remainder of \emph{XIST}. A-D) Individual points within each protein represent biological replicates. }
\label{fig:xistproteins}
\end{figure}

To do this, we compared eCLIP signal for all proteins with data in the ENCODE database inside each tandem repeat relative to the remainder of the \emph{XIST} transcript (Methods 4.5, Xist Enriched Proteins). We found that each repeat within \emph{XIST} contained at least one protein that was at least 2x more enriched in within the repeat than the remainder of the \emph{XIST} transcript (Figure \ref{fig:xistproteins}A). RBM15 and SRSF1 were found to be the most specific proteins to repeat A. RBM15 is required for full \emph{XIST} silencing functionality \cite{Moindrot2015ASilencing}, and that RBM15 recruits the m6A complex to the 5' region of \emph{XIST} \cite{Patil2016M6Repression}. SRSF1 has previously been shown to bind the A-repeat of \emph{XIST} but its function within the sequence is currently unknown.


Both the B-repeat and D-repeat were found to be significantly enriched for HNRNPK over the rest of the transcript (Figure \ref{fig:xistproteins}B-C). HNRNPK is required for recruitment of PRC1 to the \emph{XIST} transcripts \cite{Pintacuda2017HnRNPKSilencing}. Deletion of the B-repeat region has previously been shown to be sufficient to abrogate \emph{XIST} dependent PRC recruitment, as well as \emph{XIST} mediated silencing. Our analysis showed that DROSHA is also enriched within the B-repeat of \emph{XIST}, however no previous studies have identified DROSHA as a direct interactor with \emph{XIST} and so the function of this interaction is unknown.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline 
\emph{Xist} Repeat&eCLIP Data                      \\
\hline 
    A&RBM15,SRSF1    \\
      B & HNRNPK\\
      D  & HNRNPK\\
      E & TIA1,MATR3,PTBP1\\
\hline 
\end{tabular}
\caption{Most enriched RBPs for each tandem repeat domain in \emph{XIST}. }
\label{tbl:eclipscan}
\end{table}

The E-repeat of \emph{XIST} was highly enriched for several proteins. The E-repeat itself is composed primarily of T-rich sequence, and therefore the proteins we observe often contain similar motifs (\cite{Ray2013ARegulation}). PTBP1, MATR3, TARDBP, and TIA1 were all found to be the most enriched within the E-repeat. PTBP1 has previously been shown to be required for proper \emph{XIST} expression and splicing during development \cite{Stork2019DevelopmentalSplicing}. TARDBP depletion is also associated with increased expression of mis-spliced \emph{XIST} transcripts \cite{Stork2019DevelopmentalSplicing}. A recent study found that MATR3 and PTBP1 form a condensate mediated by the \emph{XIST} E-repeat and this condensate is required for \emph{XIST} localization to the Xi \cite{Pandya-Jones2020AnSilencing}. 

\subsection{Detection of Rsx Domains}
Our \emph{Rsx} study in Chapter 2 relied on the existence of tandem repeats within the \emph{Rsx} transcript to perform the domain based SEEKR analysis. On a whole transcript level, \emph{XIST} and \emph{Rsx} were slightly anti-correlated despite their shared function. The relationship between the two sequences didn't become apparent until we parsed out the tandem repeats within \emph{Rsx} and performed pairwise comparisons between each \emph{XIST} tandem repeat. 

We hypothesized that we could use \emph{hmmSEEKR} to identify regions of non-linear sequence similarity between \emph{XIST} and \emph{Rsx} without \emph{a priori} identification of tandem repeats. To do this, we trained 4 separate HMMs on the A,B,D, and E-repeats of \emph{XIST} at $k\in\{2,3,4,5,6\}$ and all pairwise combinations of $\alpha,\beta \in \{.5,.75,.9,.99,.999,.9999\}$. The results outlined in Figure \ref{fig:koalarsxhmm} and Table \ref{tbl:rsxresults} are using the set of parameters that yielded the best F1 score (Methods 3.4.X). 


\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{images/koalarsx.pdf}
\caption[HMM analysis of \emph{Rsx}]{\emph{hmmSEEKR} hits within koala \emph{Rsx} for an HMM trained on each \emph{XIST} repeat. Blue regions represent hits to the query, whereas dashed grey lines represent null sequence. The parameter vectors $\theta = (k,\alpha,\beta)$ for each repeat A,B,D,E used as a query were: $\theta_A = (4,.9999,.9999)$, $\theta_B = (4,.9999,.9999)$, $\theta_D = (2,.9999,.9999)$, $\theta_E = (4,.9999,.9999)$.}
\label{fig:koalarsxhmm}
\end{figure}

We found that the HMM trained on the \emph{XIST} D-repeat ($\theta_D = (2,.9999,.9999)$)  captured the entire tandem repeat from transcript coordinates 1,000-14,000 (Figure \ref{fig:koalarsxhmm}, Table \ref{tbl:rsxresults}). Likewise, the HMM trained on the \emph{XIST} B-repeat ($\theta_B = (4,.9999,.9999)$) captured a significant portion of \emph{Rsx} repeat 1, and these regions were significantly more C-rich than the remainder of \emph{Rsx} repeat 1. Additional C-rich sequence were also identified at the 5' and 3' regions of \emph{Rsx} (Figure \ref{fig:koalarsxhmm}). The HMMs trained on the A-repeat ($\theta_A = (4,.9999,.9999)$) and E-repeat ($\theta_E = (4,.9999,.9999)$) both significantly aligned with \emph{Rsx} repeat 4, with 100\% precision and 25.45\% recall of the full-length repeat 4. In our original analysis, we mapped A,E-repeats to repeats 2,3, and 4 in \emph{Rsx}, however repeats 2 and 3 had relatively marginal correlation to the A,E-repeats, whereas the A,E$\rightarrow$4 relationship was the strongest of all comparisons in the original \emph{Rsx} analysis \cite{Sprague2019NonlinearDomains}. 

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline 
\emph{Rsx} Repeat Name& Start & End   & Xist Association   & Precision& Recall                      \\
\hline 
    1&  1000   & 14000    & B,D     & 98.42\% & 100\% \\
      2&17500 & 21000  & A,E     & 100\%   & 1.32\% \\
      3&21500  & 22500   & A,E      & - & 0\%  \\
      4&23000 & 27500  & A,E     & 100\%   & 25.45\% \\
\hline 
\end{tabular}
\caption[\textit{Rsx-Xist} Precision and Recall]{Precision and recall for each HMM using the tandem repeat definitions from \cite{Sprague2019NonlinearDomains}. Precision is defined as the number of correct nucleotides relative to the number of incorrect nucleotides, whereas recall is the fraction of the total domain retrieved by the HMM. The parameter vectors $\theta = (k,\alpha,\beta)$ for each repeat A,B,D,E used as a query were: $\theta_A = (4,.9999,.9999)$, $\theta_B = (4,.9999,.9999)$, $\theta_D = (2,.9999,.9999)$, $\theta_E = (4,.9999,.9999)$.}
\label{tbl:rsxresults}
\end{table}


\subsection{Sequence based prediction of RBP binding in \textit{KCNQ1OT1}}

There is at least one conserved lncRNA in the mammalian transcriptome that is known to silence gene expression in \emph{cis} through a PRC-mediated mechanism similar to \emph{XIST}, \emph{KCNQ1OT1}. The mouse \emph{Kcnq1ot1} has been shown to silence megabase scale regions of chromosome 7 and human \emph{KCNQ1OT1} silences a large region of chromosome 11. Unlike \emph{XIST}, the sequence of \emph{KCNQ1OT1} is not predominantly comprised of tandem repeat domains (Figure \ref{fig:kcndot}), barring a region in the 3' region of the sequence. 

\begin{figure}[h!]
\centering
\includegraphics[width=.35\textwidth]{images/kcnd.pdf}
\caption[\emph{KCNQ1OT1} dot plot alignment]{Dot plot alignment of human \emph{KCNQ1OT1} against itself using flexidot software \cite{Seibt2018FlexiDot:Analyses}. The window parameter is 20bp and the threshold is 40\%. Black regions correspond to windows that succesfully aligned against each other. }
\label{fig:kcndot}
\end{figure}

Furthermore, eCLIP data for RBPs known to be essential for \emph{XIST}'s function show that they predominantly bind in the 5' half of \emph{KCNQ1OT1}'s sequence (Figure \ref{fig:kcnproteins}). Therefore, we sought to use the \emph{hmmSEEKR} package to identify functional sub-sequences within \emph{KCNQ1OT1} that are similar in $k$-mer content to \emph{XIST} A-,B-,D-, and E-repeats. 


\begin{figure}[h!]
\centering
\includegraphics[width=.85\textwidth]{images/kcnproteins.pdf}
\caption[\emph{KCNQ1OT1} RBM15, HNRNPK, and MATR3 eCLIP tracks]{Browser tracks for RBM15 (top), HNRNPK (middle), and MATR3 (bottom) within the \emph{KCNQ1OT1} locus. Tracks represent peak intensity from ENCODE narrowPeak files for each protein, regions with no signal were not called as statistically significant peaks. }
\label{fig:kcnproteins}
\end{figure}

\emph{KCNQ1OT1} contained regions of similarity to each of the \emph{XIST} HMMs that we trained (Figure \ref{fig:kcntracks} A-D) with parameterizations $\theta_A = (4,.9999,.9999), \theta_B = (4,.9999,.9999), \theta_D = (2,.9999,.9999), \theta_E = (4,.9999,.9999)$. We found that the 5' region of \emph{KCNQ1OT1}'s sequence in particular containins regions of similarity to A-,B-,D-, and E-repeats of \emph{XIST} (Figure \ref{fig:kcntracks} A-D) -- suggesting that this region of \emph{KCNQ1OT1} could be a multipurpose binder of numerous RBPs. Indeed, eCLIP data for RBM15 and SRSF1 (Figure \ref{fig:kcntracks} A), HNRNPK (Figure \ref{fig:kcntracks} B-C), and MATR3 and PTBP1 (Figure \ref{fig:kcntracks} D) reveal elevated read density for all these proteins at the 5' region of \emph{KCNQ1OT1}'s sequence. Additionally, a large portion of the inner region of \emph{KCNQ1OT1}'s sequence was identified by \emph{hmmSEEKR} to have similarity to the B- and D- repeats of \emph{Xist}. (Figure \ref{fig:kcntracks} B-C). Similar to actual B- and D-repeats of \emph{XIST}, these regions within \emph{KCNQ1OT1} correspond to elevated HNRNPK read density (Figure \ref{fig:kcntracks} B,C). 
\begin{figure}[h!]
\centering
\includegraphics[height=.75\textheight]{images/kcneclip.pdf}
\caption[Analysis of HMM and eCLIP data in \emph{KCNQ1OT1}]{\emph{hmmSEEKR} predicts \emph{XIST} associated protein binding in \emph{KCNQ1OT1}. A) HMM viterbi parse showing hits to the A-repeat in \emph{KCNQ1OT1} (blue), RBM15 eCLIP read density (middle plot), and SRSF1 eCLIP read density (bottom plot). B) HMM viterbi parse showing hits to the B-repeat (top, blue regions), and HNRNPK eCLIP read density (bottom). C) HMM viterbi parse in \emph{KCNQ1OT1} for the D-repeat (top, blue regions) and HNRNPK eCLIP read density (bottom). D) HMM viterbi parse for E-repeat in \emph{KCNQ1OT1} (top, blue regions) and eCLIP read density for MATR3 (middle) and PTBP1 (bottom). E-J) Total eCLIP reads assigned to the HMM or randomized parses for RBM15, SRSF1, HNRNPK (B-repeat), HNRNPK (D-repeat), MATR3, and PTBP1 respectively. HMM parameterizations were $\theta_A = (4,.9999,.9999), \theta_B = (4,.9999,.9999), \theta_D = (2,.9999,.9999), \theta_E = (4,.9999,.9999)$ for A-,B-,D-,and E- respectively. }
\label{fig:kcntracks}
\end{figure}

We then sought to compare the read density for each protein in Table \ref{tbl:eclipscan} found within our \emph{hmmSEEKR} prediction for the associated \emph{XIST} query against a randomized shuffling of the hits found within \emph{KCNQ1OT1} (Method 3.4.6 randomization). We found that the A-repeat HMM significantly outperformed randomized shuffles for both RBM15 (Figure \ref{fig:kcntracks} E; $p< 10^{-8}$, chi-square test) and SRSF1 (Figure \ref{fig:kcntracks} F; $p< 10^{-5}$, chi-square test). HNRNPK was also significantly better predicted by the HMMs trained on the B- and D-repeats of \emph{XIST} (Figure \ref{fig:kcntracks} G,H;$p<10^{-33}$ B-repeat, $p<10^{-51}$ D-repeat, chi-square test). Finally, the E-repeat trained HMM significantly outperformed a shuffled parse for both MATR3 (Figure \ref{fig:kcntracks} I, $p<10^{-28}$, chi-square test) and PTBP1 (Figure \ref{fig:kcntracks} J; $p<10^{-43}$, chi-square test). Thus, our predictions from \textit{hmmSEEKR} would support our hypothesis that \textit{KCNQ1OT1} does contain shared sequence features with \textit{XIST}. Under the traditional hypotheses of sequence alignment, \textit{XIST} and \textit{KCNQ1OT1} have no shared sequence features \cite{Altschul1990BasicTool,Wheeler2013Nhmmer:HMMs,Rice2000EMBOSS:Suite}, however using a $k$-mer based similarity metric we have identified $>10^4$ basepairs of sequence in \textit{KCNQ1OT1} that map to functional domains in \textit{XIST}. 

\clearpage


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Transcriptome-wide RBP Prediction}
Many of the proteins that bind to \emph{XIST} and \emph{KCNQ1OT1} bind throughout the transcriptome \cite{VanNostrand2016RobusteCLIP}. Furthermore, \emph{XIST} clusters with thousands of transcripts in the human genome based on $k$-mer content \cite{Kirk2018FunctionalContent}. Therefore, we hypothesized that \emph{XIST}-like sub-sequences may be found throughout the transcriptome, both in lncRNAs as well as in pre-mRNAs. To test this hypothesis, we trained 4 HMMs on the A-,B-,D-, and E-repeats of human \emph{XIST} using parameters in Table \ref{tbl:transparams} (Methods 4.1, Transition parameters) and used \emph{hmmSEEKR} to scan the set of all unspliced coding and non-coding transcripts in the human genome. The Viterbi parses for the A-,B-,D-, and E-repeat HMMs extracted sequences from the transcriptome that were significantly more similar to the query compared to the unparsed sequences (Figure \ref{fig:hmmdist}).  We then compared the results of our HMM against ENCODE eCLIP data for proteins that we have shown are enriched within each \emph{XIST} repeat (Table \ref{tbl:eclipscan}). 

\begin{figure}[h!]
\centering
\includegraphics[width=.9\textwidth]{images/hmmdist.pdf}
\caption[\emph{hmmSEEKR} extracts sub-sequences with high correlation to query]{\emph{hmmSEEKR} models trained on \emph{XIST} tandem repeats extract sub-sequences with higher SEEKR correlation to the query than unparsed sequences using parameters listed in Table \ref{tbl:transparams}. A) A-repeat HMM hits relative to unparsed sequences, B) B-repeat HMM hits relative to unparsed sequences. C) D-repeat HMM hits relative to unparsed sequences. D) E-repeat HMM hits relative to unparsed sequences. $x$-axis represents the SEEKR correlation from the query to each transcript. Parameterizations as in Table \ref{tbl:transparams} (Methods 4.1, Transition parameters).}
\label{fig:hmmdist}
\end{figure}

The \emph{hmmSEEKR} HMMs significantly out-performed randomized HMM parses for each of the proteins we tested. The A-repeat trained HMM successfully captured significantly more reads for RBM15 (Figure \ref{fig:reads} A left; $p\ll 10^{-100}$, chi-squared test) and SRSF1 (Figure \ref{fig:reads} B; $p\ll 10^{-100}$, chi-squared test) than did the randomized hits. Furthermore, true HMM hits throughout the transcriptome had significantly more reads per hit than the shuffled parses for RBM15 eCLIP data as well as significantly more reads per hit than three RBPs that are not known to associate with \textit{XIST}: UTP3, RBM22, and ZC3H8 (Figure \ref{fig:transcriptome} A; $p\ll10^{-100}$, student's t-test) and SRSF1 (Figure \ref{fig:transcriptome} B; $p\ll10^{-100}$, student's t-test).

\begin{figure}[h!]
\centering
\includegraphics[width=.7\textwidth]{images/transcriptomescan.pdf}
\caption[\textit{hmmSEEKR} outperforms random shuffle]{\textit{hmmSEEKR} models trained on XIST tandem repeats predict RBP binding regions throughout the transcriptome. A-G) Total number of eCLIP reads assigned to HMM Viterbi parse hits compared to randomized shuffling of hits within each transcript for RBM15 (A), SRSF1 (B), HNRNPK B-repeat (C), HNRNPK D-repeat (D), MATR3 (E), TIA1 (F), PTBP1 (G) Parameterizations as in Table \ref{tbl:transparams} (Methods 4.1, Transition parameters).}
\label{fig:reads}
\end{figure}

The B-repeat trained model had significantly more reads assigned from HNRNPK eCLIP data than the random parse (Figure \ref{fig:reads} C, $p\ll10^{-100}$, chi-squared test), and significantly more reads per hit than the randomized parses (Figure \ref{fig:transcriptome} C, $p\ll10^{-100}$, student's t-test). The B-repeat model is the only HMM that has a smaller mean of $\log_2$ read counts than UTP3 per hit (1.09, 1.24 respectively; $p<10^{-52}$ student's t-test), but had significantly more reads per hit than RBM22 and ZC3H8 ($p\ll 10^{-100}$, student's t-test). The D-repeat HMM captured more reads than the B-repeat HMM (Figure \ref{fig:reads} C-D), and overlapped significantly more reads than randomized parses (Figure \ref{fig:reads} D, $p\ll10^{-100}$, chi-square test) as well as more reads per hit than the randomized parses and RBM22, ZC3H8, and UTP3 (Figure \ref{fig:transcriptome} D, $p\ll10^{-100}$, student's t-test). 

\begin{figure}[h!]
\centering
\includegraphics[width=.95\textwidth]{images/plot.pdf}
\caption[Transcriptome-wide prediction of RBP binding regions]{\emph{hmmSEEKR} models trained on \emph{XIST} tandem repeats predict binding sites of \textit{XIST} associated RBPs throughout the transcriptome. A-G) $\log_2$ read count per hit distribution for the true HMM hits compared to the shuffled parses, for RBM15 (A), SRSF1 (B), HNRNPK B-repeat (C), HNRNPK D-repeat (D), MATR3 (E), TIA1 (F), PTBP1 (G). Also shown are the distribution of $\log_2$ read counts for non-\textit{XIST} associated proteins RBM22, ZC3H8, and UTP3. Parameterizations as in Table \ref{tbl:transparams} (Methods 4.1, Transition parameters).}
\label{fig:transcriptome}
\end{figure}

Finally, the E-repeat trained HMM also successfully predicted associated protein binding better than randomized parses (Figure \ref{fig:transcriptome} E-G). MATR3 had significantly more reads assigned by the true HMM (Figure \ref{fig:reads} E, $p\ll 10^{-100}$, chi-square test) and more reads per hit than random (Figure \ref{fig:transcriptome} E, $p\ll 10^{-100}$, student's t-test). The E-repeat trained HMM assigned significantly more reads from TIA1 eCLIP (Figure \ref{fig:reads} F, $p\ll 10^{-100}$, chi-squared test) and the number of reads per hit for the true HMM parses and the randomized parses was significantly higher (Figure \ref{fig:transcriptome} F, $p\ll 10^{-100}$, student's t-test). PTBP1 displayed the highest ratio of assigned reads between the true HMM and randomized parses over all comparisons (Figure \ref{fig:reads} A-G) and assigned significantly more eCLIP reads than the randomized parses (Figure \ref{fig:reads} G, $p\ll10^{-100}$, chi-squared test) as well as more reads per hit compared to random (Figure \ref{fig:transcriptome} G, $p\ll10^{-100}$, student's t-test). MATR3, PTBP1, and TIA1 all had significantly more reads per hit in the HMM parse compared to the RBPs that do not associate with \textit{XIST} ($p\ll 10^{-100}$, student's t-test). These data show that domains with $k$-mer based similarity to \textit{XIST} in the transcriptome are binding \textit{XIST}-associated proteins in a manner consistent with the hypotheses in \cite{Kirk2018FunctionalContent,Sprague2019NonlinearDomains}.  

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%


\section{Discussion}
The non-coding portion of our genome is a vast and nebulous network of RNA transcripts whose functions are largely unknown -- and if they are, their mechanism(s) of action have proven difficult to understand. A major obstacle to better understanding lncRNA function is the relatively unknown sequence-to-function relationship in lncRNAs. In particular, even the traditional principles of conservation analysis seem to provide little insight into lncRNAs \cite{Johnsson2014EvolutionaryFunction,Pang2006RapidFunction,Nesterova2001CharacterizationSequence}. Therefore, an entirely different toolbox is needed if computational prediction of functional lncRNAs are to be made.

Our lab has recently developed a $k$-mer based algorithm for quantification of sequence similarities between non-coding transcripts \cite{Kirk2018FunctionalContent}. This method was built for comparison of full-length sequences, however it is clear from the relationship between \emph{XIST} and \emph{Rsx} that much of \emph{XIST}'s sequence bears no resemblance to \emph{Rsx} \cite{Sprague2019NonlinearDomains}, and often only sub-sequences within a larger lncRNA may be conserved between species \cite{Pang2006RapidFunction,Nesterova2001CharacterizationSequence}. A statistical model was necessary to identify where within a lncRNA sequence there are regions of non-linear sequence similarity between the lncRNA and a query domain that has some \emph{a priori} known function, \textit{e.g.} the tandem repeats of \textit{XIST}. 

Here, we have developed a python package, \emph{hmmSEEKR}, that uses an HMM to parse sequences of interest into regions that either have $k$-mer based similarity to some known functional domain, or not. \emph{hmmSEEKR} utilizes the underlying model of lncRNA sequence functionality in SEEKR \cite{Kirk2018FunctionalContent} and hypothesizes that the functionality of lncRNA can be localized within functional modules \cite{Pang2006RapidFunction, Nesterova2001CharacterizationSequence,Hezroni2015PrinciplesSpecies,Brockdorff2018LocalNcRNA,Sprague2019NonlinearDomains} in order to more precisely pin down the sequence relationship between two lncRNAs. 

We found that \emph{hmmSEEKR} was able to reproduce the non-linear sequence relationship between \emph{XIST} and \emph{Rsx} without the \emph{a priori} extraction of tandem repeat domains from \emph{Rsx} (\cite{Sprague2019NonlinearDomains}, Figure \ref{fig:koalarsxhmm}). Additionally, \emph{hmmSEEKR} identified regions of non-linear sequence similarity between \emph{XIST} and \emph{KCNQ1OT1} for each of the \emph{XIST} functional domains that an HMM was trained on. This indicates that \emph{KCNQ1OT1} contains the necessary sequence information bind the RBPs required for PRC mediated silencing, which we verified with statistical analysis of ENCODE eCLIP data \cite{VanNostrand2016RobusteCLIP}. Finally, we found that \emph{hmmSEEKR} models trained on A-,B-,D-, and E- repeats within \emph{XIST} overlapped with significantly more eCLIP reads for each query's associated proteins (Table \ref{tbl:eclipscan}) than randomly generated parses, and in all cases except TIA1 the eCLIP reads per hit were significantly higher for the \emph{hmmSEEKR} parses than the randomized parses. \emph{hmmSEEKR} was therefore able to predict functional binding locations for \emph{XIST}-associated proteins throughout the transcriptome. 

There may be numerous transcripts in our genome that contain the necessary information to function similarly to \textit{XIST}, \textit{Rsx}, and \textit{KCNQ1OT1}. \textit{hmmSEEKR} represents the first tool designed to consider functional analogy rather than homology when modeling function between RNA transcripts. Going forward, we hope to computationally predict and experimentally verify transcripts that \textit{hmmSEEKR} identifies as containing the necessary information to silence through PRC recruitment. 


%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%


\section{Methods}

\subsection{Parameter Estimation}
A hidden markov model is comprised of a stochastic transition matrix $A$, where each row vector $\psi_i$ corresponds to a categorical distribution representing the probability of hidden state $i$ transitioning to state $j$. The emission matrix $E$, where each row corresponds to a categorical distribution of $k$-mer frequencies. Finally, there is the initialization matrix $\pi$ that describes the probability of starting at each hidden state at $t=1$.
\subsubsection{Emission Parameters}
We defined the emission distribution for the query and null hidden states to be $k$-mer frequencies taken from a set of training sequences. For the query hidden state, we defined our training sets to be \emph{XIST} repeats A,B,D, and E. For the null hidden state, we defined the set of training sequences to be the set of unspliced lncRNAs in the human transcriptome. A separate HMM was trained for each \emph{XIST} repeat. 

Some of the \emph{XIST} repeats are quite short, for example \emph{XIST} repeat B is only $\approx 200$bp in length. Therefore, it is often the case that there are fewer nucleotides than there are parameters to estimate leading to zeros in the probability distribution. In reality, zero probability $k$-mers are both unlikely, as many of the motifs for RBPs that bind \emph{XIST} have no non-zero probabilities, and also mathematically intractable, as log-space for probability calculations is required for long sequences.

To correct for this parameterization problem we employed pseudo-counts, specifically we use a $+1$ pseudo-count for all $k$-mers in the training set (\emph{i.e.} initialize the counts array to 1). The hypothesis driving the pseudo-count is that despite the limited training data implying zero probability events, this only occurs because there is insufficient data to sample enough $k$-mers and their frequencies. Given the transition matrix $A$, each row $i$ (representing a hidden state's emission distribution) within $A$ is a categorical distribution $\psi_i $ comprised of the frequencies for each possible $k$-mer. It can be shown that the \emph{maximum a posteriori}, or MAP, estimate for the $k$-mer frequency distribution $\psi_i$ is achieved by adding a pseudo-count of $1$ to each $k$-mer \cite{Rabiner1989ARecognition}. This is achieved by placing a Dirichlet prior on the $k$-mer frequency distribution:

\begin{center}
    $\psi_i \sim Dir(\alpha_i^{AAA}+1,\alpha_i^{AAT}+1,\dots,\alpha_i^{GGG}+1)$
    
    $x_i \sim Cat(\psi_i^{AAA},\psi_i^{AAT},\dots,\psi_i^{GGG})$
\end{center}

The $\alpha$ parameters in the Dirichlet distribution would be the observed counts of each $k$-mer, and the +1 represents the pseudo-count. As the $\alpha$ for a given $k$-mer increases, the probability ``concentration" around that $k$-mer increases -- so when a sample of $\psi_i$ is drawn -- the $k$-mers with high $\alpha$ parameters are going to have the highest frequencies, and therefore most likely to be drawn when an observation $x_i$ is drawn. That is to say, the best estimate of the $k$-mer frequencies, given the data available and the uncertainty in it, is achieved by adding a count of $1$ to all $k$-mers. Intuitively this makes sense, as this addition has the largest impact when there is very little data to work with (less than $4^k$ $k$-mers to train from), and very little impact when there is substantial data as is the case for the null hidden state, which is trained on many thousands of $k$-mers.

\subsubsection{Transition Parameters and $k$}

We used the SEEKR algorithm to calculate a score for how well the HMM performed at identifying query-like sequence regions based off their $k$-mer similarity to the original query.  For a given set of parameters, the Kullback-Leibler diveregence was calculated between a SEEKR correlation distributions of parsed and unparsed sequences, by running \emph{hmmSEEKR} on all transcripts in the human unspliced transcriptome and comparing the Viterbi ``hits" to the original, unspliced transcripts. The KL-divergence is a metric of ``distance" between two probability distributions. 

The KL-Divergence is defined as \cite{Brookes1951FoundationsProbability}:
\begin{equation}
    D_{KL}(P||Q) = \sum_{x\in X}P(x)\log{\frac{P(x)}{Q(x}}
\end{equation}
Where $D_{KL}$ is the KL-Divergence between two probability distributions P and Q. P represents the SEEKR score distribution of the HMM parse ``hits" (contiguously ``+'' labeled $k$-mers within the sequence) to the query sequence, and Q represent the SEEKR score distribution of unparsed sequences to the query sequence, representing a ‘reference’. If $P = Q$ then $D_{KL} = 0$. D increases as the probability distributions become more dissimilar to each other. $D_{KL}(P||Q) \neq D_{KL}(Q||P)$, so for consistency $D_{KL}(P||Q)$ was always calculated. Being comprised of unparsed sequences, $Q(x)$ just depends on $k$.
In the real dataset, $P$ and $Q$ are histograms calculated from the data, and so the calculation of $D_{KL}$ compares the frequencies of SEEKR correlations in each bin for the two distributions. The larger the difference between $P$ and $Q$, the more successful the HMM was at parsing out highly correlated regions to the query.

The goal was to find a set of parameters, $\alpha$ ($+\rightarrow +$), $\beta$ ($-\rightarrow -$), and $k$, that maximize the expression in equation \ref{eq:transeq}.
\begin{equation}
    \argmax_{\alpha,\beta,k}\left[\mu_S\sum_{x\in X}P(x|\alpha,\beta,k)\log{\frac{P(x|\alpha,\beta,k)}{Q(x|k)}}\right]
\label{eq:transeq}
\end{equation}

Where $\mu_S$ is the average HMM score (log-likelihood of the sequence, Methods 4.2 ``HMM Score") over all hits in the transcriptome, $P(x)$ is the distribution of SEEKR scores for parsed out hits, and $Q(x)$ is the distribution of SEEKR scores for full length unspliced transcripts in the human transcriptome: 

\begin{table}[h]
\centering
\begin{center}
 \begin{tabular}{|c | c| c | c |} 
 \hline
 Query & $k$ & $\alpha$ & $\beta$ \\
 \hline\hline
 Repeat A & 4 & .9999 & .9999 \\ 
 \hline
 Repeat B & 4 & .9999 & .9999\\
 \hline
 Repeat D & 2 & .75 & .9999\\
 \hline
 Repeat E & 3 & .5 & .9999\\
 \hline
\end{tabular}
\end{center}
\caption[KL-Divergence best fit parameters]{Best values of $k$, the $+\rightarrow +$ parameter $\alpha$, and $-\rightarrow -$ parameter $\beta$ as determined through a grid search based approach to identify sequences with the highest SEEKR correlation to the query.}
\label{tbl:transparams}
\end{table}

We found that $\beta$ must always be close to 1, whereas significant variance was found for the value of $\alpha$ amongst the different \emph{XIST} queries (Table \ref{tbl:transparams}). We also found that there was often significant overlap of hits between the varying sets of parameters for each query, implying that there was general consistency regardless of general parameterization. (Note: Repeat A, $k=5$ scored higher than $k=4$, however, the HMM model for $k=5$ has a positive expectation for the log-likelihood matrix between query and null. All score based sequence models generally require a negative expectation \cite{Altschul1990BasicTool} to prevent spurious ``hits'' due to random chance, and simulations we performed indicated this was a problem). 
\subsection{Pseudo-code and algorithms}
The functions that drive much of analysis in the \emph{hmmSEEKR} package are found in the corefunctions.py file. These include the algorithms behind HMM analysis including the forward, backward, Viterbi, and Baum-Welch algorithm implementations. In addition, several functions crucial for parsing the supplied DNA sequences and outputting in human readable format are found within this file.

\subsubsection{$k$-mer counting}
$k$-mer counting is at the heart of both SEEKR and hmmSEEKR. This calculation is one of the simplest, but potentially most time consuming portions of the analysis. This is largely because there is no way around reading along the supplied string(s), and counting every $k$-mer as encountered. To speed $k$-mer counting up, \textbf{Algorithm \ref{alg:kmercounts}} has been implented using the cython package within Python, which is a library that allows for C-like implementation of python code.

The pseudo-code in Algorithm \ref{alg:kmercounts} is implemented in kmers.py and kmers.pyx and takes a FASTA file as input. The program then saves a dictionary, or hash map, with $k$-mers as keys and their counts as values is created, and as each $k$-mer is encountered within the sequence, that $k$-mer's count is incremented by one.

\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\KwResult{Python dictionary of $k$-mer counts}
 read in fasta file\;
 strFasta $\leftarrow$ concactenate fasta strings with delimiter character\;
 intL $\leftarrow$ calculate total length excluding delimiter\;
 dictKmerMap $\leftarrow$ initialize dictionary of $k$-mers with initial counts of 1\;
 \For{$k$-mer in sequence}{
  \eIf{$k$-mer in dictKmerMap }{
   dictKmerMap[$k$-mer]$\leftarrow$increment $k$-mer by 1
   }{
   do not increment\;
  }
 }
 Save binary file of python dictionary containing counts
 \caption{Counting $k$-mers from supplied sequences}
 \label{alg:kmercounts}
\end{algorithm}

\subsubsection{Ambiguous Nucleotides}
Transcripts containing regions of ambiguous sequence are of concern when building the viterbi parse, as ambiguous $k$-mers are not included in the emission distributions of the hidden state. Therefore only sequences of $k$-mers containing A,T,C, or G are allowed. To adjust for any ambiguous positions within the sequence, \textbf{Algorithm \ref{alg:ambigIdx}} stores two lists of indices. The first stores the position, or index, of each allowable $k$-mer, and the second stores the position of any $k$-mer containining an ``N'' within it. If there are any ``N" within the sequence, the list of $k$-mers passed into the Viterbi algorithm are split into distinct sub-sequences, because the regions flanking the ambiguous nucleotides are not adjacent to each other. Finally, when mapping the Viterbi parse back to the original sequence, the two lists of indices generated above are used to merge the results back to their original order.

An example would best illustrate what this portion of the code is trying to achieve. For simplicity, let $k=1$, and our observation be $X= A,A,N,N,T,T$. Counting indices from 1, the allowable $k$-mer indices in $X$ are $\{1,2,5,6\}$. The ambiguous $k$-mer indices are $\{3,4\}$. As the sequence $X$ is fragmented by two Ns at position 3 and 4, the flanking sequences are passed to the Viterbi algorithm to be parsed separately: $\{\{A,A\},\{T,T\}\}$. The grouping algorithm then sorts the viterbi parse back in with the ambiguous nucleotides, $\{+,+,N,N,-,-\}$.

\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\KwResult{Sequence of unambiguous $k$-mers}
\emph{listO}$\leftarrow$initialize empty list to contain $k$-mers\;
\emph{listIdx}$\leftarrow$initialize empty list to contain $k$-mer indices\;
\emph{listAmbigIdx}$\leftarrow$initialize empty list to contain ambig $k$-mer locations\;
\emph{listKmers}$\leftarrow$list of $k$-mers that can be constructed from ATCG\;
\emph{intIndex}$\leftarrow$0\;
 \For{k-mer \textbf{in} sequence}{
 \eIf{k-mer \textbf{in} listKmers}{
 Append \emph{$k$-mer} to \emph{listO}\;
 Append \emph{intIndex} to \emph{listIdx}\;}{
 Append \emph{intIndex} to \emph{listAmbigIdx}\;
 }
 \emph{intIndex}$+1$\;
 }
 \textbf{return} \emph{listO} \emph{listIdx} \emph{listAmbigIdx}\;
 \caption{Generate unambiguous observed sequence}
 \label{alg:ambigIdx}
\end{algorithm}
\subsubsection{Identifying hits from the Viterbi parse}
Within \emph{hmmSEEKR} a Viterbi parse is a sequence of state labels that have been assigned to each $k$-mer from a DNA sequence, \emph{e.g.} if the DNA sequence was ``AAAACCCC", and $k=4$, then the $k$-mer sequence is $\{AAAA,AAAC,AACC,ACCC,CCCC\}$, and a hypothetical Viterbi parse would be a list such as $\{-,-,-,+,+\}$, where ``$-$" represents the null state, and ``$+$" represents the query state. \textbf{Algorithm \ref{alg:groupHMM}} takes the list $\{-,-,-,+,+\}$ and converts it into a grouped list, \emph{e.g.} $\{\{-,-,-\},\{+,+\}\}$. This also exactly defines what term a hit within our HMM, consecutively labeled ``$+$" regions with a sequence from the viterbi parse.
\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\KwResult{Viterbi backtrack grouped into consecutive states}
\emph{condition}$\leftarrow$Swap condition, \emph{i.e.} '-' encountered\;
\emph{Key}$\leftarrow$Stores a FLAG that switches value when condition is met\;
\emph{previousNuc}$\leftarrow$Previous nucleotide\;
\For{nucleotide in sequence}{
\emph{currNucBool}$\leftarrow$TRUE/FALSE is \emph{nucleotide} in \emph{condition}\;
\emph{prevNucBool}$\leftarrow$TRUE/FALSE is \emph{previousNuc} in \emph{condition}\;
\eIf{currNucBool$\neq$prevNucBool}{swap \emph{Key}\;}{\emph{Key}\;}
\emph{previousNuc}$\leftarrow$nucleotide\;
}
 \caption{Group HMM hits}
 \label{alg:groupHMM}
\end{algorithm}

\subsubsection{HMM Score}
The score for the HMM is calculated for each ``hit" within the transcript, as defined in the prior section. We assume that the set of state labels from the Viterbi algorithm are now known. This allows, for a hit $X$ of length $L$, the likelihood that $X$ was emitted from the query state over its entire length. 

$$P(X=\{x_1,\dots,x_L\} | Y = \{+,\dots,+\}) = \prod_{i=1}^L{p(x_i|+)}$$

This likelihood can be compared to the likelihood of $X$ had it been emitted from the null state, and a likelihood ratio can be calculated.

$$S' = \frac{\prod_{i=1}^L{p(x_i|+)}}{\prod_{i=1}^Lp(x_i|-)}$$

Due to the lengths of most sequences, these calculations are moved to log-space, yielding the formula for the score reported by \emph{hmmSEEKR}. These calculations are implemented in Algorithm \ref{alg:LLR}.

$$S = \sum_{i=1}^L{\left[\log_2 p(x_i|+)- \log_2 p(x_i|-)\right]}$$
\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{arrLLR}$\leftarrow$empty array with an element for each hit\;
\emph{hits}$\leftarrow$list of sequences that HMM called hits\;
\emph{Query}$\leftarrow$Frequency distribution of kmers in query\;
\emph{Null}$\leftarrow$Frequency distribution of kmers in null\;

\For{hit \textbf{in} hits}{
\emph{intLLRQuery}$\leftarrow$ 0\;
\emph{intLLRNull}$\leftarrow$ 0\;
\For{kmer \textbf{in} hit}{
\emph{intLLRQuery}$+\log{P(kmer|Query)}$\;
\emph{intLLRNull}$+ \log{P(kmer|Null)}$\;
}
\emph{arrLLR[hit]}$\leftarrow$\emph{intLLRQuery}-\emph{intLLRNull}\;
}
 \caption{Log-likelihood}
 \label{alg:LLR}
\end{algorithm}

\subsubsection{Viterbi Parse}

The premise behind the Viterbi algorithm is outlined in the introduction. Briefly, the algorithm finds the sequence of hidden states that maximizes the joint probability function of the HMM defined in equation \ref{eq:jointhmm}, given the observations $X$, the transition matrix $A$, the emission matrix $E$, and the initial probabilities of each hidden state $\pi$. The viterbi algorithm is implemented in \emph{hmmSEEKR} as in \textbf{Algorithm \ref{alg:viterbi}}.

At each step in the Viterbi algorithm, the state transition that maximizes the probability at time $t$ is recorded, for each state. That is, at time $t$, the ``+" state has a most likely transition from either ``+'' or ``-" from time $t-1$, and the ``-" state at time $t$ has a most likely transition from either ``+" or ``-" from time $t-1$ in the sequence. To identify which was the most likely, a backtrace must be calculated after all these calculations have been made, and this is implemented in \textbf{Algorithm \ref{alg:backtrack}}.

\begin{algorithm}[B]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{O}$\leftarrow$Sequence of kmers\;
\emph{E}$\leftarrow$Emission matrix\;
\emph{A}$\leftarrow$Transition matrix\;
\emph{$\pi$}$\leftarrow$Starting probability of each state\;
\emph{states}$\leftarrow$ List of states \emph{i.e.} [-,+]\;
\emph{dictViterbi}$\leftarrow$List of dicts with cumulative probability at each time $t$ for each state\;
\;
\emph{dictViterbi[1][query]} means the cumulative probability at time $t=1$ at state=query\;\;
\emph{dictMaxState}$\leftarrow$List of dicts containing the state at time $t-1$ that maximized transition to each state at time $t$\;
\;
\;
Initial probabilities at each state at time $t=1$\;
\For{state \textbf{in} states}{
\emph{dictViterbi[1][state]}$\leftarrow \log{P(state|\pi)}+\log{P(O_1|state)}$
}
\;

Calculate probability of being in each state at time $t$ and find which state transition maximizes this probability\;
\For{t \textbf{in} range(2,length of O)}{\;
Add new dictionary element to dictViterbi and dictMaxState
\;
\For{state \textbf{in} states}{\;
selectedState$\leftarrow$ start with the first state to calculate prob\;
currMaxProb$\leftarrow \log P(state|selectedState)+dictViterbi[t-1][selectedState]$\;
\;
\For{checkState in remaining states}{\;
tempProb$\leftarrow \log P(state|checkState)+dictViterbi[t-1][checkState]$\;\;
\eIf{tempProb$>$currMaxProb}{set currMaxProb to tempProb\;}{keep currMaxProb the same\;selectedState$\leftarrow$checkState\;}
}
currMaxProb$\leftarrow$currMaxProb$+\log P(O_t|state)$\;
\emph{dictViterbi[t][state]}$\leftarrow$currMaxProb\;
\emph{dictMaxState[t][state]}$\leftarrow$selectedState\;
}
}
 \caption{Viterbi parse}
 \label{alg:viterbi}
\end{algorithm}

\cleardoublepage

\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined

\emph{backtrack}$\leftarrow$empty list to append maximizing states to\;
\emph{dictViterbi}$\leftarrow$List of dicts with cumulative probability at each time $t$ for each state\;
\;
\emph{dictViterbi[1][query]} means the cumulative probability at time $t=1$ at state=query\;\;
\emph{dictMaxState}$\leftarrow$List of dicts containing the state at time $t-1$ that maximized transition to each state at time $t$\;
\;
\;
maxProbability$\leftarrow$largest probability at time $t=T$\;
maxState$\leftarrow$state associated with maxProbability\;
maxPriorState$\leftarrow$dictMaxState[T][maxState], get state that transitioned to maxState at $t=T-1$\;
\;
Append maxPriorState to backtrack\;

\For{t=T-2 to t=1}{
Append dictMaxState[t+1][maxPriorState] to backtrack\;
maxPriorState$\leftarrow$dictMaxState[t+1][maxPriorState]
}

\textbf{return} backtrack with order flipped to start at $t=1$

 \caption{Backtrack viterbi}
 \label{alg:backtrack}
\end{algorithm}
\clearpage
\subsubsection{Forward Algorithm}
The forward algorithm calculates the likelihood of the data, $P(X)$, as well as the probability of the sequence $x_{1:t}$ ending at state $i$ at time $t$. The algorithm was implemented through the pseudo-code below in \emph{hmmSEEKR}. \\
\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{X}$\leftarrow$Sequence of kmers\;
\emph{Y}$\leftarrow$Sequence of hidden states\;
\emph{E}$\leftarrow$Emission matrix\;
\emph{A}$\leftarrow$Transition matrix\;
\emph{$\pi$}$\leftarrow$Starting probability of each state\;
\emph{states}$\leftarrow$ List of states \emph{i.e.} [-,+]\;
\;

$\alpha_i(t)\leftarrow$forward probabilities, indexed by time $t$ and state $i$ \emph{i.e.} the probability of being in state $i$ at time $t$\;
$T\leftarrow$length of X\;\; 

Initialize probabilities at $t=1$\;
$\alpha_i(t=1)\leftarrow \pi(i) + \log P(X_t |Y_t = i)$\;\;
\For{t in T}{
Append new entry to $\alpha$\;
\For{state "i" in states}{
currP$\leftarrow$\{+:0,-:0\} : dict with entry for each state, indexed by state name\;
\For{state "j" in states}{
$A_{ij}(t)\leftarrow P(Y_t=i|Y_{t-1}=j)$\;
currP[j]$\leftarrow \alpha_j(t-1) + A_{ij}(t)$\;
}
$\alpha_i(t)\leftarrow \textrm{logsumexp}\left(currP\right)+\log P(X_t|Y_t=i)$\;
}
}
 \caption{Forward Algorithm}
 \label{alg:fwd}
\end{algorithm}
\clearpage
\subsubsection{Backward Algorithm}
The backward algorithm calculates the probability of the observed sequence $P(X)$, as well as the probability of sequence $x_{t+1:T}$ starting in state $i$ at time $t$. The algorithm was implemented as below in \textit{hmmSEEKR} and is primarily used in the Baum-Welch algorithm. \\
\begin{algorithm}[h]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{X}$\leftarrow$Sequence of kmers\;
\emph{Y}$\leftarrow$Sequence of hidden states\;
\emph{E}$\leftarrow$Emission matrix\;
\emph{A}$\leftarrow$Transition matrix\;
\emph{$\pi$}$\leftarrow$Starting probability of each state\;
\emph{states}$\leftarrow$ List of states \emph{i.e.} [-,+]\;
\;

$\beta_i(t)\leftarrow$backward probabilities, indexed by time $t$ and state $i$ \emph{i.e.} the probability of being in state $i$ at time $t$\;
$T\leftarrow$length of X\;\; 

Initialize probabilities at $t=T$\;
$\beta_i(t=T)\leftarrow 0$ ; probability of 1 in log space\;\;
\For{T to t=1}{
Append new entry to $\beta$\;
\For{state "i" in states}{
currP$\leftarrow$\{+:0,-:0\} : dict with entry for each state, indexed by state name\;
\For{state "j" in states}{
$A_{ij}(t)\leftarrow P(Y_{t+1}=j|Y_{t}=i)$\;
$E_j(t+1)\leftarrow P(X_{t+1}|Y_{t+1}=j)$\;
currP[j]$\leftarrow \beta_j(t+1) + A_{ij}(t) + E_j(t+1)$\;
}
$\beta_i(t)\leftarrow \textrm{logsumexp}\left(currP\right)$\;
}
}

\textbf{return} $\beta$ inverted s.t. $1\rightarrow T$ rather than $T \rightarrow 1$\;
 \caption{Backward Algorithm}
 \label{alg:bwd}
\end{algorithm}

\subsubsection{Baum-Welch Algorithm}
This algorithm updates the transition matrix parameters. The premise of this algorithm is outlined in the introduction. Briefly, the BW algorithm calculates the expected number of ``observations" of each hidden state $i$, as well as the expected number of ``observed" transitions from each hidden state $i$ to all possible states $j$ (which may include $i$, and in \textit{hmmSEEKR} does). 

\begin{algorithm}[H]
\DontPrintSemicolon
\setstretch{1}
\SetAlgoLined
\emph{X}$\leftarrow$Sequence of kmers\;
\emph{Y}$\leftarrow$Sequence of hidden states\;
\emph{E}$\leftarrow$Emission matrix\;
\emph{A}$\leftarrow$Transition matrix\;
\emph{$\pi$}$\leftarrow$Starting probability of each state\;
\emph{states}$\leftarrow$ List of states \emph{i.e.} [-,+]\;
\;

$\alpha(t)\leftarrow$forward probabilities\;
$\beta(t)\leftarrow$backward probabilities\;\;

$\gamma_{i}(t)\leftarrow \frac{P(Y_t=i,X|A,E,\pi)}{P(X_t)}$ ; probability of being in state i at time t\;
$\epsilon_{ij}(t)\leftarrow \frac{P(Y_t=i,Y_{t+1}=j,X|A,E,\pi)}{P(X_t)}$ ; probability of being in state i at time t and state j at time t+1\;
\;

\For{each time "t" in sequence X}{
\For{state "i" in states}{
$\gamma_i(t)\leftarrow \alpha_i(t)+\beta_i(t) - \textrm{logsumexp}\left(\alpha_i(t)+\beta_i(t)\right)$\;
\For{state "j" in state}{
$A_{ij}(t+1)\leftarrow \log P(Y_{t+1}=j|Y_t=i)$\;
$E_j(t+1)\leftarrow \log P(X_{t+1}|Y_{t+1}=j)$\;
numerator$\leftarrow \alpha_i(t)+A_{ij}(t) + \beta_j(t+1)+E_j(t+1)$\;
denominator$\leftarrow \log P(X_t)$\;\;
$\epsilon_{ij}(t)\leftarrow$numerator-denominator\;
}
}
Append new entry to $\gamma$ list of dicts\;
Append new entry to $\epsilon$ list of dicts\;
}

Sum $\gamma$ and $\epsilon$ over all values of $T$ for all states $i$ and $j$. The sum below represents the expected number of transitions from state $i$ to state $j$ over the sequence. 
$$A^*_{ij} = \frac{\sum_t{\epsilon_{ij}(t)}}{\sum_t{\gamma_{i}(t)}} $$\;

Do not update emission matrix $E$.\;\;
\textbf{return} $A^*$\;

 \caption{Baum Welch Parameter Update}
 \label{alg:bw}
\end{algorithm}

\clearpage

\subsection{Python implementation}
\begin{table}[t!]
\centering
 \begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
 \hline
 Program & Input & Purpose\\
 \hline\hline
 kmers.py & fasta file & Count $k$-mers within fasta files provided \\ 
 \hline
 train.py & counts files from kmers.py for query and null & Generate matrices that define the HMM\\
 \hline
 mSEEKR.py & fasta file for sequence(s) of interest, path to output of train.py & retrieve the viterbi parse of the sequence of interest to identify potential functional domains \\
 \hline
bw.py & HMM training sequences, initial parameterizations & provide MLE of transition parameters \\
 \hline
 
\end{tabular}
\caption{Individual programs within the \emph{hmmSEEKR} package.}
\label{programs}
\end{table}
\emph{hmmSEEKR} has been implemented as a python package in order to rapidly train HMMs and scan sequences of interested for potential function domains. Given the general algorithms defined above, which roughly correspond to individual functions within the \emph{hmmSEEKR} package, a thorough guide is provided for how the algorithms have been implemented. 

\subsubsection{Installation}
Table \ref{programs} provides an overview of the different python programs within the \emph{hmmSEEKR} package. The first step is to ensure that Python3.6 or greater is installed, as well as the SEEKR package and hmmSEEKR repository which can be retrieved entering the following commands:

\begin{verbatim}
> pip install seekr
> git clone https://github.com/spragud2/mSEEKR
> cd mSEEKR/
> python setup.py build_ext --inplace
\end{verbatim}

\subsubsection{$k$-mer frequencies}
To count $k$-mers for a set of sequences, a single fasta file containing any number of sequences is required. The program counts the $k$-mers in all sequences, and then calculates the average over all of them to provide a single distribution of $k$-mer frequencies. Additionally the program is capable of multi-processing over different values of $k$, such that 2-,3-,4-,$\dots$-mers can be calculated simultaneously. 
\begin{table}[h]
\centering
 \begin{tabular}{|l l|}
 \hline
 Parameter & Function\\
 \hline
 --fasta & Path to fasta file \\
 --name & Output file name \\
 --dir & Output directory \\
 -k & Comma delimited list of values of $k$ \emph{e.g.} 2,3,4,5\\
 -a & Alphabet to use \emph{e.g.} ATCG\\
 -n & Number of processors to use\\
 \hline
 
\end{tabular}
\caption{Parameters for kmers.py}
\label{tab:kmerparams}
\end{table}

The program counts $k$-mers as defined in Algorithm \ref{alg:kmercounts}. The following python code reads in the parameters defined above. The provided fasta file is converted into a singular string with a delimiter character \$, and the total length of the string excluding the delimiter is calculated. Finally, the script kmers.pyx, which contains the implementation of Algorithm \ref{alg:kmercounts}, is spooled onto the number of processors passed, and the results are compiled into a dictionary. 

\lstinline{> python kmers.py --fasta ./fastaFiles/gencode.vM17.lncRNA_transcripts.fa -k 2,3,4 --name mm10Trscpts -n 3}

The results are then saved into a binary file containing a python dictionary containing $k$-mer frequencies for each value of $k$ specified in the arguments. 
\lstinputlisting[language=Python]{code/kmersMain.py}
\subsubsection{HMM Creation}
To create the HMM files necessary for running the mSEEKR.py program, the train.py defines the matrices $A,E,\pi$ that are defined in Algorithms \ref{alg:viterbi}, \ref{alg:fwd}, \ref{alg:bwd}, and saves them in a single binary file. The emission probabilities are equivalent to the $k$-mer frequencies calculated from kmers.py. Due to restrictions of available training data for our own experiments, the user can manually provide the self-transition parameters for the query and null hideen states.

If training sequences are available, the --bw flag can be passed to run the Baum-Welch algorithm to obtain an MLE for the transition parameters. If the --bw flag is passed, the --iter argument must be passed. The results from the --bw are then passed directly into the main train.py program. To better check the results of a Baum-Welch operation it is recommended to run the bw.py program separately with a variety of initial parameterizations. This allows for inspection of whether local minima or a potential global maximum have been reached. The results from the previous experiment can then be manually passed to train.py --qT and --nT arguments.
\begin{table}[h]
\centering
 \begin{tabular}{|l l|}
 \hline
 Parameter & Function\\
 \hline
 --query & Path to kmer counts of query \\
 --null & Path to kmer counts of null \\
 --qT & Query$\rightarrow$Query transition probability \\
 --nT & Null$\rightarrow$Null transition probability \\
 --qPrefix & Name of query for file path \\
 --nPrefix & Name of null for file path\\
 --dir & Directory to put HMM model \\
 -k & Comma delimited list of values of $k$ \emph{e.g.} 2,3,4,5\\
 -a & Alphabet to use \emph{e.g.} ATCG\\
 \hline
 
\end{tabular}
\caption{train.py parameters}
\label{tab:trainparams}
\end{table}
\linebreak

\emph{hmmSEEKR} creates directories for the save files in a predefined fashion to ensure proper retrieval of the correct matrices when running mSEEKR.py. The following code checks to see if the directory specified in --dir exists, and if not, creates the directory. 

\lstinline{> python train.py --query ./counts/mouseA.skr --null ./counts/mm10Trscpts.skr -k 2,3,4 --qPrefix mouseA --nPrefix mm10Trscpts --qT .9999 --nT .9999 --dir ./markovModels/}

\lstinputlisting[language=Python]{code/trainDir.py}
The script then loads the $k$-mer counts specified in --query and --null and passes and begins looping through the values of $k$ specified in the -k argument. The following code loops through each value of $k$, loads the $k$-mer frequencies, and creates the matrices $A,E,\pi$ in the form of a python dictionary. The 2-state dimensions of this HMM are hard-coded into the script. 

\lstinputlisting[language=Python]{code/trainSave.py}
The resulting python dictionaries are then saved into a binary file in a dictionary within the directory matching the following pattern:

\begin{verbatim}
    --dir/--qPrefix_--nPrefix/-k/hmm.mkv
\end{verbatim}
\subsection{Viterbi Parsing}
The primary script within the $hmmSEEKR$ is mSEEKR.py. This script takes the matrices $A,E,\pi$ created from train.py and scans a sequence, or set of sequences, and calculates the most likely parse, or Viterbi path, through each sequence. Therefore a sequence, \emph{e.g.} ATCG, is converted into an equal length string of state lebels, \emph{e.g.} - - + -. The program then extracts consecutively + (query) labeled nucleotides, and reports each such region as a \emph{hit}. 

\lstinline{> python mSEEKR.py --db ./fastaFiles/mm10kcn.fa -n 8 --prefix test --model ./markovModels/mouseA_mm10Trscpts -k 3 --fasta}

\begin{table}[h]
\centering
 \begin{tabular}{|l l|}
 \hline
 Parameter & Function\\
 \hline
 --model & Path to .mkv file output from train.py or bw.py \\
 -k & Length of short motif (k-mer) \\
 --db & Sequence database to use, currently accepts FASTA format\\
 --prefix & Output file name prefix \\
 -a & Alphabet to use \emph{e.g.} ATCG \\
 -n & Number of processors to use (default=1)\\
 --fasta & Include sequence in output \\
 \hline
 
\end{tabular}
\caption{mSEEKR.py parameters}
\label{tab:viterbiparams}
\end{table}

The following code section from mSEEKR.py loads in the specified HMM model from the --model argument, and ensures that the path has been provided in the correct format, \emph{i.e.} ending with a ``/". 
\lstinputlisting[language=Python]{code/mSEEKRLoad.py}

The sequences to be parsed are then spooled onto the number of threads provided in the -n argument, and that is executed by this code:

\lstinputlisting[language=Python]{code/mSEEKRpool.py}

Each sequence is then prepared for input into the Viterbi algorithm. First, the sequences are passed to the function \texttt{kmersWithAmbigIndex}, which implements the pseudo-code outlined in Algorithm \ref{alg:ambigIdx}. The next step is the calculation of the Viterbi parse, as outlined in Algorithm \ref{alg:viterbi}. 

\lstinputlisting[language=Python]{code/mSEEKRcalc.py}

The parsed sequences are then combined with any ambiguous sequence regions from the original supplied sequence such that the original sequence is reconstructed, but with hidden state labels instead of nucleotides. 

Finally the regions that have been designated as ``hits" are compiled in a dataframe within the \texttt{formatHits} and \texttt{hitOutput} functions. Additionally, this is where the HMM score is calculated as in Algorithm \ref{alg:LLR}. 

\subsection{bw.py - Transition parameter optimization}
A key feature of \textit{hmmSEEKR} is that it searches target transcripts for a very specific query that is defined \textit{a priori}. \textit{hmmSEEKR} obtains the query $k$-mer frequency estimates using the maximum likelihood estimate plus a pseudocount from the training sequences. The transition parameters, however, are less well defined or have no clear calculation from the data. Therefore, we opted to use an iterative expectation-maximization scheme while keeping the emission parameters, \textit{i.e.} the $k$-mer frequencies, fixed. 

To do this, a custom implementation of the Baum-Welch algorithm was written as part of the \textit{hmmSEEKR} package. The algorithm works by alternating between the expectation of $P(Y | X, \theta)$, and then updating the parameters in a way that is guaranteed to increase the likelihood of the model, relative to the previous set of parameters, up to a local maximum. 

Within the context of an HMM and the transition parameters, this corresponds to calculating the expected number of observations belonging to each hidden state $i$ in the training sequence, as well as the expected number of hidden state transitions $i \rightarrow j$, where $j$ can be any allowable transition from the transition matrix $A$. From here, these expectations are used to update the parameters $\theta$. This is done by taking the partial derivative of the complete log-likelihood function with respect to each parameter and solving for 0. These update rules are outlined in the introduction as well as in algorithm \ref{alg:bw}. 


\begin{table}[h]
\centering
 \begin{tabular}{|l l|}
 \hline
 Parameter & Function\\
 \hline
 --prior & Path to binary .mkv file output from train.py \\
 -k & Length of short motif (k-mer) \\
 --db & Sequence database to use, currently accepts FASTA format\\
 --createfile & Create new file rather than overwrite\\
 --its & Number of iterations to run BW algorithm \\
 \hline
 
\end{tabular}
\caption{bw.py parameters}
\label{tab:bwparams}
\end{table}

Example usage: 

\lstinline{> python bw.py -k 4 --db ./fastaFiles/xist.fa --prior markovModels/mouseA_mm10Trscpts/4/hmm.mkv --its 3 -cf}

The algorithm is implemented in python as in algorithms \ref{alg:fwd}, \ref{alg:bwd}, \ref{alg:bw}. Initially, the bw.py script reads in an initially trained model that contains guesses for the parameters $\alpha$ and $\beta$. The script then checks the sequence for any ambiguous nucleotides, and if any exist, the sequence is split into contiguous chunks of unambiguous sequence. From here, a dictionary is created that will track the values of the parameters $\alpha$ and $\beta$ over each iteration of the BW algorithm. 

\lstinputlisting[language=Python]{code/bw1.py}

Following this, the main loop of the BW algorithm begins. First, the forward and backward probabilities are calculated for the entire training sequence. The updated parameters $\alpha$ and $\beta$ are then calculated. Last, these parameters are normalized to sum to one (to be a proper probability distribution), and the loop continues until the specified number of iterations is completed. 

\lstinputlisting[language=Python]{code/bw2.py}

From here, two files are saved. One contains the values of $\alpha$ and $\beta$ over each iteration of the BW algorithm. This file should be checked for convergence of the parameters. Second, a new binary file containing the updated HMM is saved, and can be passed to mSEEKR.py. 

\lstinputlisting[language=Python]{code/bw3.py}


\subsection{Xist Enriched Proteins}

All ENCODE eCLIP Experiments (non-controls) in K562 cells were selected and downloaded from the ENCODE database (bigWig file type). 

\lstinline{xargs -L 1 curl -O -L < encodefileurls.txt\\
for i in *bigWig; do ./bigWigToBedGraph $i ${i}.bedGraph;done}



We then calculated the ratio of average eCLIP signal within Xist Repeats to the average signal outside repeats. 

We defined $X$ as a set of size $L = 19275$, where each entry corresponds to a basepair in \emph{XIST}. Each nucleotide within this set, $x_i$, was given a value depending on whether or not eCLIP signal was present from the bigWig files retrieved from ENCODE. If there was no signal, a value of 0 was given to $x_i$, otherwise the read density from the bigWig file was assigned to $x_i$.

We calculated the eCLIP signal enrichment of each protein within each \emph{XIST} repeat by using the transcript coordinates of each repeat in \emph{XIST} to define a set $I$, which contains all individual nucleotide positions within a given \emph{XIST} repeat. Set $I^c$ is the complement of $I$, relative to $X$,  and contains all basepairs in \emph{XIST} that are not in $I$. 

If $N$ is the size of $I$, \emph{i.e.} the size of the given repeat in \emph{XIST}, then the ratio of average signal $R$ is:

$$R = \frac{L-N}{N}\frac{\sum_{i\in{I}}{X_i}}{{\sum_{j\in{I^c}}{X_j}}}$$

\subsection{\emph{Rsx} HMM Analysis}

Human \emph{XIST} repeats A,B,D,E were used as queries within 4 separate \emph{hmmSEEKR} models. For the null model, human unspliced lncRNAs (GENCODE V26) were used. A model was trained for each value of $k \in \{2,3,4,5,6\}$ and for the transition parameters all combinations of the following values were tested, $\alpha,\beta \in \{.5, .75, .9, .99, .999, .9999$\}. The koala \emph{Rsx} transcript \cite{Johnson2018AdaptationGenome} was then scanned using each of these HMMs. For measuring performance of the HMM, precision was defined as the number of nucleotides from the HMM Viterbi parse that fell within the correct \emph{Rsx} repeat as found in \cite{Sprague2019NonlinearDomains}. Recall was defined as the percentage of each \emph{Rsx} repeat recovered from the associated \emph{XIST} trained HMM. The $F_1$ score was then calculated as 

\begin{equation}
    F_1 = \left(\frac{2}{\texttt{precision}^{-1}+\texttt{recall}{^-1}}\right)
\end{equation}

And the set of parameters for each \emph{XIST} query with the highest $F_1$ score was chosen (Table \ref{tbl:rsxparams}.

\begin{table}[h]
\centering
\begin{center}
 \begin{tabular}{|l| l| l | l |} 
 \hline
 Query & k & $\alpha$ & $\beta$ \\
 \hline
 Repeat A & 4 & .9999 & .9999 \\ 
 \hline
 Repeat B & 4 & .9999 & .9999\\
 \hline
 Repeat D & 2 & .9999 & .9999\\
 \hline
 Repeat E & 4 & .9999 & .9999\\
 \hline
\end{tabular}
\end{center}
\caption[\emph{Rsx} F1-score best parameters]{Best values of k, the $+\rightarrow +$ parameter $\alpha$, and $-\rightarrow -$ parameter $\beta$ as determined through a grid search based approach to identify sequences with the highest SEEKR correlation to the query.}
\label{tbl:rsxparams}
\end{table}

\subsection{\textit{KCNQ1OT1} Analysis}

\subsubsection{HMM Training}
HMMs were trained on each of the human \emph{XIST} A-,B-,D-, and E- repeats \cite{Brown10TheNucleus.} with parameters $\theta = (k,\alpha,\beta)$. The A-repeat had parameters $\theta_A = (4,.9999,.9999)$, B-repeat $\theta_B = (4,.9999,.9999)$, D-repeat $\theta_D = (2,.9999,.9999)$, E-repeat $\theta_E = (3,.9999,.9999)$. The unspliced human KCNQ1OT1 gene (GENCODE V26) was provided as the ``target'' sequence to be parsed. 

\subsubsection{Randomization}

Randomized parses were generated for each query's results by taking each hit $i$ after scanning \textit{KCNQ1OT1} and generating a random point between $[1,L-l_i+1]$ (where $l_i$ is the length of hit $i$). The hit was then ``moved" to this new location within the transcript. This process was repeated for each hit. 
\subsubsection{eCLIP Read overlap}

To calculate eCLIP signal, we used featureCounts ( \texttt{-s 2 -F SAF -p} options) to overlap the reads from each proteins BAM file with the genomic coordinates of the HMM hits, and randomized parses. 

\subsection{Transcriptome Search}
The eCLIP files from the RBPs in Table \ref{tbl:eclipscan} were sourced from ENCODE, with the BAM files corresponding to the GRCH38 build of the human genome pulled from the following experiments: 

\begin{itemize}
  \item RBM15 - ENCFF739LLZ
  \item SRSF1 - ENCFF696TJG
  \item HNRNPK - ENCFF894NKS
  \item TIA1 - ENCFF080VML
  \item MATR3 - ENCFF162SAS
  \item PTBP1 - ENCFF765BPN
  \item RBM22 - ENCFF045LAO
  \item ZC3H8 - ENCFF409GUM
  \item UTP3 - ENCFF826UUS
\end{itemize}

ENCODE chromatin association RNA-seq data (experiment ID: ENCFF957VGD) was used to identify expressed transcripts that are enriched around chromatin. The following programs were then used to identify these expressed transcripts, parse sequences of interest, and overlap the reads from the BAM files above with the HMM parses: 

\begin{itemize}
   \item mSEEKR/v1.0.8
    \item RSEM/1.2.31
    \item deeptools/3.0.0
    \item  subread/2.0.0
\end{itemize}

The unspliced/spliced ratio for each transcript was done using RSEM. A fasta file with transcript isoforms of interest was generated. For this analysis, a single spliced isoform (primary) and the unspliced transcript were included in the fasta file from the GENCODE V26 annotation gff file. The following commands were then run: 

\lstinline{module add rsem/1.2.31}\\
\lstinline{module add samtools}\\
\lstinline{module add deeptools/2.5.2}\\
\lstinline{module add bowtie/1.2.2}\\
\lstinline{sbatch --mem 50g -t 5:00:00 -N 1 -n 24 --wrap='bowtie-build --threads 24 v26_combo.fa hgT'}\\
\lstinline{sbatch --mem 50g --wrap='rsem-prepare-reference . hgT'}\\    


After the reference was built for RSEM, the \texttt{rsem-calculate-expression} program was called to quantify the TPM of the unspliced and spliced isoforms of each transcript, where EXPNAME was replaced with the name of the ENCODE experiment being quantified. 

\lstinline{sbatch -N 1 --mem 50g -t 5:00:00 -J rsemChr -n 24 --wrap='rsem-calculate-expression -p 24 --paired-end READS1.fastq READS2.fastq  hgT EXPNAME'}\\

Following this, transcripts with unspliced isoforms above a threshold TPM ($> \log_2$ TPM 0) were used as input into \emph{hmmSEEKR}. Parameterizations for the A-,B-,D-, and E-repeat HMMs were as in Table \ref{tbl:transparams}, with the null models trained on unspliced human lncRNAs (GENCODE V26).  

A SAF (Simple Annotation Format) file was then created from the \emph{hmmSEEKR} output. A SAF file is similar to BED6 input with re-arranged columns. We then used feature counts to quantify the read overlaps between the \emph{hmmSEEKR} SAF file and the ENCODE BAM files for each protein. 

In tandem with the real \emph{hmmSEEKR} parse, a randomized parse for each transcript was created (as in the KCNQ1OT1 analysis). For each hit within a transcript a random point was chosen with uniform probability within the transcript from position 1, to the final coordinate less the length of the hit. This randomly chosen point was then designated as the start point for the random hit, with length equal to the original hit from \emph{hmmSEEKR}. From here, a SAF file was created for all randomized hits and overlapped with the ENCODE eCLIP data. 

We then calculated the total number of reads retrieved by the true HMM and the randomized parses, as well as the average number of reads per hit. Statistics were calculated using a chi-square test for total reads retrieved, with a null hypothesis of 1:1 ratio between the true HMM and the randomized parses, and a student's t-test of the average number of reads per hit with a null hypothesis of no difference between HMM and random. 

\clearpage 

\titleformat{\section}
  {\centering\normalfont\fontsize{12}{12}\bfseries}{\thesection}{1em}{}
\begin{singlespace}

\printbibliography[heading=subbibintoc,title={REFERENCES}]

\end{singlespace}

\titleformat{\section}
  {\normalfont\fontsize{12}{12}\bfseries}{\thesection}{1em}{}